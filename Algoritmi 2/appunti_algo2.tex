\documentclass[11pt]{article}
\usepackage[margin=.8in]{geometry}
\usepackage[italian]{babel}
\usepackage{tikz}
\usepackage{amsfonts} 
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{thmtools}
\usepackage[Algoritmo]{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{hyperref}


\title{Appunti Algoritmi 2}

\newtheorem*{theorem}{Teorema}
\newtheorem*{proprietà}{Proprietà}
\newtheorem*{lemma}{Lemma}
\newtheorem*{corollary}{Corollario}
\renewcommand{\listalgorithmname}{Algoritmi}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,      
    urlcolor=cyan,
    }

\urlstyle{same}

\begin{document}
\pagenumbering{roman}
\tableofcontents
\listofalgorithms
\newpage
\pagenumbering{arabic}
\section{Grafi come strutture dati}
\subsection{Introduzione e terminologia}
Un grafo è una coppia di elementi (insiemi) \textbf{G=(V,E)} e consiste in:
\begin{itemize}
    \item un insieme $V$ di \textbf{vertici} (o \textbf{nodi})
    \item un insieme $E$ ($E$ sottoinsieme del prodotto cartesiano $V\times V$) di coppie di vertici, detti \textbf{archi}
    o \textbf{spigoli}; ogni arco connette due vertici
\end{itemize}
I grafi possono essere:
\begin{itemize}
    \item \textbf{orientati}: relazioni asimmetriche, insieme di coppie ordinate
    \item \textbf{non orientati}: relazioni simmetriche, insieme di coppie non ordinate
\end{itemize}
Un arco è \textbf{incidente} per i nodi che si toccano.\\
Il \textbf{grado} di un vertice è dato dal numero di archi incidenti.\\
Un vertice $B$ è \textbf{adiacente} ad $A$ se da $B$ si può percorrere un solo arco e giungere ad $A$.\\
Un \textbf{sottografo} è una porzione di grafo (notazione $H\subseteq G$): i vertici di $H$ sono sottoinsieme dei vertici di $G$ 
e gli archi di $H$ sono sottoinsieme degli archi di $G$.\\
Un \textbf{cammino} è una sequenza ordinata di archi che collegano due nodi. I cammini devono rispettare l'orientamento 
degli archi. La \textbf{lunghezza} è il numero di archi di cui è composto un cammino.\\ Un cammino si dice \textbf{semplice} 
se non passa due volte per lo stesso vertice. Se esiste almeno un cammino $p$ tra i vertici $v$ e $w$, si dice che $w$ è 
\textbf{raggiungibile} da $v$. Inoltre $v$ è un \textbf{antenato} di $w$ e $w$ è un \textbf{discendente} di $v$.\\ 
Un cammino tra due nodi $v$ e $w$ si dice \textbf{minimo} se tra $v$ e $w$ non esiste nessun altro cammino di lunghezza 
minore. La lunghezza del cammino minimo è detta \textbf{distanza} ($\delta (v,w)$).\\
Un grafo può essere \textbf{pesato}. La funzione peso è definita come $W:E\rightarrow \mathbb{R}$; per ogni arco
$(v,w)\in E,W(v,w)$ definisce il \textbf{peso} di $(v,w)$. In un grafo pesato, la lunghezza/peso di un cammino si calcola 
sommando i pesi degli archi che contiene.\\
I grafi non orientati possono essere:
\begin{itemize}
    \item \textbf{connessi}: esiste un cammino da ogni vertice verso ogni altro vertice
    \item \textbf{non connessi}
\end{itemize}
I grafi orientati possono essere:
\begin{itemize}
    \item \textbf{fortemente connessi}: esiste un cammino da ogni vertice verso ogni altro vertice
    \item \textbf{debolmente connessi}: ignorando il verso degli archi
\end{itemize}
Un cammino $\langle w_1,w_2,...,w_n \rangle$ si dice \textbf{chiuso} se $w_1=w_n$. Un cammino chiuso, semplice, di lunghezza 
almeno 1 si dice \textbf{ciclo}. Se un grafo non contiene cicli, si dice \textbf{aciclico}.\\
Un \textbf{grafo completo} è un grafo con un arco per ogni coppia di vertici. Un grafo completo ha numero di archi $E$ pari 
a $|E| = \frac{|V|(|V|-1)}{2}$.\\
Un grafo non orientato, connesso e aciclico è definito \textbf{albero libero}. Se un vertice è designato ad essere radice, 
si definisce \textbf{albero radicato}. Un grafo non orientato, aciclico ma non connesso è definito \textbf{foresta}.
\subsection{Rappresentazione}
Per valutare un approccio di rapppresentazione, bisogna considerare lo \textbf{spazio} occupato dalla struttura dati e il 
\textbf{costo computazionale} delle operazioni da effettuare su di essa.
\subsubsection{Lista di archi}
Dati $n$ (numero di vertici) e $m$ (numero di archi), lo spazio occupato è $\mathcal{O}(n+m)$: è una rappresentazione 
inefficiente, in quanto bisogna percorrere tutto il grafo per scandire la lista di archi. Introdurre un vertice o arco ha 
costo $\mathcal{O}(1)$, ma la rimozione ha costo $\mathcal{O}(m)$.
\subsubsection{Liste di adiacenza}
Ogni vertice $v$ ha una lista contenente i vertici ad esso adiacenti. Calcolare il grado di un vertice è un'operazione 
semplice, in quanto basta scorrere la lista di adiacenza. Occupa spazio $\mathcal{O}(n+m)$, ed è adatta per grafi \textbf{sparsi} 
(il numero di archi è molto minore del numero di vertici).
\subsubsection{Liste di incidenza}
Ogni vertice $v$ ha una lista contenente un riferimento agli archi ad esso incidenti. Occupa spazio $\mathcal{O}(n+m)$.
\subsubsection{Matrici di adiacenza}
Il grafo è rappresentato tramite una matrice di interi di grandezza $n \times n$ (spazio occupato $\mathcal{O}(n^2)$); è 
adatta per grafi \textbf{densi}. Calcolare il grado e archi incidenti ha costo $\mathcal{O}(n)$ (basta scorrere la matrice). 
La modifica dei vertici ha costo $\mathcal{O}(n^2)$ in quanto bisogna ricostruire completamente la matrice.
Una matrice di adiacenza rappresenta anche la presenza di un cammino di lunghezza 1 tra ogni coppia di vertici $v$ e $w$. 
In particolare, $v \rightarrow_1 w$ se e solo se $M[v,w]\neq 0$: moltiplicando la matrice per sè stessa, il risultato è 
diverso da 0 solo se esiste un cammino di lunghezza 2 (e via dicendo).
\subsubsection{Matrici di incidenza}
Il grafo è rappresentato tramite una matrice di interi di grandezza $n \times m$ (spazio occupato $\mathcal{O}(n \times m)$), 
in cui le righe indicizzano i vertici e le colonne indicizzano gli archi.
\section{Visite}
\subsection{Visita generica}
Una \textbf{visita} di un grafo $G$ permette di esaminare i nodi e gli archi in maniera sistematica, senza passare due 
volte per lo stesso nodo.
\subsubsection{Inizializzazione}
Una tattica per evitare di visitare un nodo più volte è quella di mappare lo stato della visita ad un colore:
\begin{itemize}
    \item \textbf{bianco} (o \textbf{nodi inesplorati}): vertice non ancora esplorato
    \item \textbf{grigio} (o \textbf{nodi aperti}): vertice visitato, ma con nodi adiacenti ancora inesplorati
    \item \textbf{nero} (o \textbf{nodi chiusi}): vertice visitato, con adiacenti esplorati
\end{itemize}
Dati $n$ nodi, si utilizza un vettore \textit{color} di colori, di grandezza $n$: all'inizio della visita, tutte le celle 
del vettore \textit{color} sono impostate a \textit{white}.
\begin{algorithm}[H]
    \caption{INIZIALIZZA(G)}
    \begin{algorithmic}
        \State color $\gets$ vettore di lunghezza n
        \For{ogni $u\in V$}
        \algstore{1}
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[h]
    \begin{algorithmic}
        \algrestore{1}
            \State color[$u$] $\gets$ white
        \EndFor
    \end{algorithmic}
\end{algorithm}
La visita parte da un nodo $s$, detto \textbf{nodo sorgente}.
\begin{algorithm}[H]
    \caption{VISITA(G,s)}
    \begin{algorithmic}
        \State INIZIALIZZA(G)
        \State color $\gets$ gray
        \State \{visita $s$\}
        \While{ci sono vertici grigi}
            \State $u \gets$ scegli un vertice grigio
            \If{esiste $v$ bianco adiacente ad $u$}
                \State color[$v$] $\gets$ gray
                \State \{visita $v$\}
            \Else{ color[$v$] $\gets$ black}
            \EndIf
        \EndWhile
    \end{algorithmic}
\end{algorithm}
Il cambiamento di colore è \textbf{monotono} (bianco $\rightarrow$ grigio $\rightarrow$ nero).
\subsubsection{Invarianti}
Un'\textbf{invariante} è una condizione che è verificabile come vera sia all'inizio sia alla fine di un ciclo:
\begin{itemize}
    \item Invariante 1: se esiste un arco $(u,v)\in E$ ed $u$ è nero, allora $v$ è grigio o nero 
    \item Invariante 2: tutti i vertici grigi o neri sono raggiungibili dalla sorgente 
    \item Invariante 3: qualunque cammino dalla sorgente ad un vertice bianco deve contenere almeno un vertice grigio 
\end{itemize}
\begin{theorem}
    Al termine dell'algoritmo di visita, $v$ è nero se e solo se $v$ è raggiungibile dalla sorgente.
\end{theorem}
\begin{proof}
    Per l'invariante 2, all'uscita dal ciclo tutti i vertici neri sono raggiungibili da $s$. Dall'invariante 
    3 si ricava che tra $s$ e $v$ esiste almeno un vertice grigio, oppure $v$ non è bianco. Dato che la condizione di uscita 
    dal ciclo è quella che non esistano più vertici grigi, si ricava che $v$ non è bianco (cambiamento monotono) e non può 
    essere grigio. Quindi, all'uscita dal ciclo, tutti i vertici raggiungibili dalla sorgente sono neri.
\end{proof}
\subsubsection{Predecessori}
L'algoritmo può essere modificato in modo da ricordare, per ogni vertice che viene scoperto, quale vertice grigio ha permesso 
di scoprirlo, ossia ricordare l'arco percorso. Ad ogni vertice $u$ si associa un attributo $\pi[u]$ che rappresenta il 
vertice che ha permesso di scoprirlo.
\begin{algorithm}[H]
    \caption{VISITA(G,s)}
    \begin{algorithmic}
        \State INIZIALIZZA(G)
        \State color $\gets$ gray
        \State \{visita $s$\}
        \While{ci sono vertici grigi}        
        \algstore{3}
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
    \begin{algorithmic}
        \algrestore{3}
        \State $u \gets$ scegli un vertice grigio
            \If{esiste $v$ bianco adiacente ad $u$}
                \State color[$v$] $\gets$ gray
                \State $\pi[v] \gets u$
                \State \{visita $v$\}
            \Else{ color[$v$] $\gets$ black}
            \EndIf
        \EndWhile
    \end{algorithmic}
\end{algorithm}
\begin{proprietà}
    Al termine dell'esecuzione di VISITA(G,s), tutti e soli i vertici neri diversi da $s$ hanno predecessore diverso da 
    NULL.
\end{proprietà}
Il sottografo dei predecessori è un albero (\textbf{albero dei predecessori}) di radice $s$.\\
Se il grafo non è connesso:
\begin{algorithm}[H]
    \caption{VISITA TUTTI I VERTICI(G)}
    \begin{algorithmic}
        \State INIZIALIZZA(G)
        \For{ogni $u\in V$}
            \If{color[$u$] = white} 
            \State VISITA(G,u)
            \EndIf
        \EndFor
    \end{algorithmic}
\end{algorithm}
\subsubsection{Gestione dei vertici grigi}
Per gestire i nodi grigi si usa una struttura dati ordinata $D$ (\textbf{frangia}). Sulla frangia è possibile eseguire le 
seguenti operazioni:
\begin{itemize}
    \item \textbf{Create()}: restituisce una $D$ vuota
    \item \textbf{Add(D,x)}: aggiunge un elemento x a $D$
    \item \textbf{First(D)}: restituisce il primo elemento di $D$
    \item \textbf{RemoveFirst(D)}: elimina il primo elemento di $D$
    \item \textbf{NotEmpty(D)}: restituisce vero se D contiene almeno un elemento, falso altrimenti
\end{itemize}
$D$ è una \textbf{coda} se Add(D,x) aggiunge l'elemento in coda a $D$, uno \textbf{stack} se Add(D,x) aggiunge l'elemento 
in testa a $D$.
\begin{algorithm}[H]
    \caption{VISITA(G,s)}
    \begin{algorithmic}
        \State INIZIALIZZA(G)
        \State Create() 
        \State color[$s$] $\gets$ gray
        \State \{visita $s$\}
        \State Add(D,s)
        \While{NotEmpty(D)}
        \State $u \gets$ First(D)
        \If{esiste $v$ bianco adiacente ad $u$}
            \State color[$v$] $\gets$ gray
        \algstore{5}
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[h]
    \begin{algorithmic}
        \algrestore{5}
            \State $\pi[v] \gets u$
            \State \{visita $v$\}
            \State Add(D,v)
        \Else
            \State color[$v$] $\gets$ black
            \State RemoveFirst(D)
        \EndIf
    \EndWhile
    \end{algorithmic}
\end{algorithm}
\subsubsection{Complessità}
Il costo di visita è $\mathcal{O}(n+adj)$: $adj$ è il tempo impiegato a controllare se esiste un nodo $v$ bianco adiacente 
ad $u$, e dipende dalla rappresentazione; $n$ è il numero di vertici, che vengono inseriti e rimossi da $D$.\\
Il costo di $adj$ è:
\begin{itemize}
    \item con lista di archi: bisogna scandire l'intera lista ($\mathcal{O}(m)$) per $n$ volte ($\mathcal{O}(n)$), quindi 
    $\mathcal{O}(n)+\mathcal{O}(n*m)=\mathcal{O}(mn)$
    \item con matrice di adiacenza: bisogna scandire l'intera riga della matrice ($\mathcal{O}(n)$), quindi $\mathcal{O}(n)+\mathcal{O}(n*n)=\mathcal{O}(n^2)$
    \item con liste di adiacenza: si possono ottimizzare le prestazioni utilizzando dei puntatori che puntano all'inizio 
    delle liste di adiacenza. Se l'elemento è grigio, il puntatore è spostato all'elemento successivo; quando il puntatore 
    giunge alla fine della lista, il primo elemento è colorato di nero. Ogni lista è percorsa una volta sola, in tutte le 
    iterazioni del ciclo. Complessità: $\mathcal{O}(n+m)$.
\end{itemize}
\subsection{Visita in ampiezza}
\subsubsection{Inizializzazione}
La \textbf{visita in ampiezza} (\textbf{BFS}, Breadth First Search), esamina i vertici del grafo in un ordine ben preciso,
costruendo un albero di visita chiamato \textbf{albero BFS}. Nell'albero BFS, ogni vertice si trova il più vicino possibile 
alla radice. La visita è realizzata usando la frangia come coda: quando un nodo grigio ha tutti gli adiacenti grigi, esso 
è rimosso dalla cosa (il vertice in testa rimane nella coda finchè non diventa nero).
\begin{algorithm}[h]
    \caption{VISITA BFS(G,s)}
    \begin{algorithmic}
        \State INIZIALIZZA(G)
        \State queue() 
        \State color[$s$] $\gets$ gray
        \State \{visita $s$\}
        \State enqueue(D,s)
        \While{NotEmpty(D)}
        \State $u \gets$ head(D)
        \If{esiste $v$ bianco adiacente ad $u$}
            \State color[$v$] $\gets$ gray
            \State $\pi[v] \gets u$
            \State \{visita $v$\}
            \State enqueue(D,v)
            \algstore{6}
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[h]
    \begin{algorithmic}
        \algrestore{6}
        \Else
            \State color[$v$] $\gets$ black
            \State dequeue(D)
        \EndIf
    \EndWhile
    \end{algorithmic}
\end{algorithm}
\subsubsection{Albero di visita}
L'albero BFS viene costruito a livelli; l'albero rappresenta i cammini minimi. Anche se le liste di adiacenza vengono 
invertite, i nodi per livello non cambiano.
Si può inizializzare un \textbf{vettore di distanze} (stimate) $d$, inizializzato ad infinito: se un determinato vertice non
è stato trovato (distanza $\infty$ a fine BFS), allora non è raggiungibile da $s$.
\subsubsection{Proprietà}
\begin{proprietà}[1]
    In D ci sono tutti e soli i vertici grigi.
\end{proprietà}
\begin{proprietà}[2]
    Se $\langle v_1,v_2,\dots ,v_n \rangle$ è il contenuto di D, allora:
    \begin{itemize}
        \item[i] $d[v_i] \leq d[v_{i+1}]$: i vertici sono ordinati per livelli nella coda
        \item[ii] $d[v_n] \leq d[v_1]+1$: la coda contiene al massimo due livelli
    \end{itemize}
\end{proprietà}
\begin{proof}
    Nel caso base, in $D$ è presente solo la sorgente. La proprietà 2 è vera.\\
    Il passo ha due casi:
    \begin{itemize}
        \item dequeue(D): o $D$ rimane vuota (banalmente vera), o rimangono $\langle v_2,\dots ,v_n \rangle$, e 
        \begin{itemize}
            \item[i.] le disuguaglianze sono ancora vere e quindi
            \item[ii.] anche $d[v_n] \leq d[v_1]+1 \leq d[v_2]+1$
        \end{itemize}
        \item enqueue(D,v): $v$ è reso figlio di $v_1$ e accodato, quindi $d[v]=d[v_1]+1$ e
        \begin{itemize}
            \item[i.] $d[v_n] \leq d[v_1]+1=d[v]$
            \item[ii.] $d[v]=d[v_1]+1 \leq d[v_1]+1$
        \end{itemize}
    \end{itemize}
\end{proof}
\subsubsection{Dimostrazione $d[v]=\delta(s,v)$}
\begin{lemma}[Invariante 4]
    $d[v]=\delta(s,v)$ per tutti i vertici grigi o neri.
\end{lemma}
\textit{\textbf{Dimostrazione $d[v] \geq \delta(s,v)$.}} Dato che l'albero dei predecessori $\pi$ contiene solo archi appartenenti 
a $G$, il cammino da $s$ a $v$ è un cammino che appartiene anche a $G$, quindi la lunghezza del cammino da $s$ a $v$ 
nell'albero è maggiore o uguale alla distanza tra $s$ e $v$.

\textit{\textbf{Dimostrazione $d[v] \leq \delta(s,v)$.}} Definiamo l'insieme dei vertici a distanza $k$ dalla sorgente nel grafo 
come $V_k={v\in V|\delta(s,v)=k}$ ($v_0$ contiene solo la sorgente).\\ Nel caso base, $d[v_0]\leq \delta(s,v)$ (distanza di 
$s$ da sè stesso: $0\leq 0$). Sia $v\in V_k$: allora $\delta(s,v)=k$ (per definizione).\\ Con $k>0$ (passo), esisterà almeno 
un vertice $w$ tale che $\delta(s,w)=k-1$ e $(w,v)\in E$, ovvero un arco che va da $v$ a $w$. Definiamo l'insieme dei vertici 
appartenenti a $V_{k-1}$ con arco entrante in $v$ come $U_{k-1}={w\in V_{k-1}|(w,v)\in E}$. Tra questi, sia $u$ il primo 
vertice di $U_{k-1}$ ad essere scoperto ed inserito nella coda: per politica FIFO, $u$ sarà anche il primo ad essere estratto
dalla coda. Quando guarderò i vertici adiacenti a $u$, $v$ sarà ancora bianco (perchè più lontano), e $v$ verrà inserito 
nell'albero come figlio di $u$, con $d[v]=d[u]+1$. Inoltre, per ipotesi induttiva, $d[u]\leq k-1$.\\
Quindi, quando inseriremo $v$ nell'albero:
\begin{itemize}
    \item $d[v]=d[u]+1$
    \item ma $d[u]\leq k-1$, quindi $d[v]\leq (k-1)+1$
    \item $d[v]\leq k$
\end{itemize}
\subsubsection{Distanza nell'albero BFS}
\begin{theorem}
    Al termine dell'esecuzione della visita BFS, si ha $d[v]=\delta(s,v)$ per tutti i vertici $v\in V$.
\end{theorem}
\begin{proof}
    caso base: se $v$ non è raggiungibile da $s$, allora $d[v]$ rimane $\infty$.\\
    altrimenti, $v$ è nero (per invariante 2). Per ogni vertice $v$ raggiungibile da $s$, il cammino da $s$ a $v$ sull'albero 
    ottenuto dalla visita è un \textbf{cammino minimo}.
\end{proof}
\subsection{Visita in profondità}
\subsubsection{Inizializzazione}
La \textbf{visita in profondità} (\textbf{DFS}, Depth First Search), esamina i vertici del grafo partendo dall'ultimo 
vertice incontrato. Si ottiene dall'algoritmo di visita generico, implementando la frangia come \textbf{stack}.
\begin{algorithm}
    \caption{VISITA DFS (ottimizzata)}
    \begin{algorithmic}
        \State D $\gets$ emptyStack(D)
        \State color[$s$] $\gets$ gray
        \State \{visita $s$\}
        \State push(D)
        \While{esiste $v$ non considerato adiacente a top(D)}
            \If{color[$v$] = white}
                \State color[$v$] = gray 
                \State $\pi[v] \gets$ top(D) 
                \State \{visita $s$\}
                \State push(D,s)
            \EndIf 
            \State color[top(D)] $\gets$ black
            \State pop(D)
        \EndWhile
    \end{algorithmic}
\end{algorithm}
\subsubsection{Caratteristiche dell'albero DFS}
Un vertice viene chiuso (colorato di nero) solo quando tutti i suoi discendenti sono stati chiusi.\\
Gli intervalli di attivazione di una qualunque coppia di vertici sono o \textbf{disgiunti} o \textbf{uno contenuto interamente 
nell'altro}. Questo è l'ordine delle attivazioni delle chiamate di una procedura ricorsiva, ed è quindi possibile costruire 
un algoritmo DFS ricorsivo. Inoltre, è possibile introdurre un contatore per ricordare l'ordine delle attivazioni.
\begin{algorithm}[H]
    \caption{VISITA DFS RICORSIVA(G,u)}
    \begin{algorithmic}
        \State color[$u$] $\gets$ gray
        \State \{visita $s$\}
        \algstore{8}
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
    \begin{algorithmic}
        \algrestore{8}
        \State d[$u$] $\gets$ time
        \State time++
        \For{ogni $v$ adiacente ad $u$}
            \If{color[$v$] = white}
                \State $\pi[v] \gets$ $u$
                \State VISITA DFS RICORSIVA(G,v)
            \EndIf 
            \State color[$u$] $\gets$ black
            \State f[$u$] $\gets$ time
            \State time++
        \EndFor
    \end{algorithmic}
\end{algorithm}
\subsubsection{Teorema delle parentesi e corollari}
\begin{theorem}[Parentesi]
    In ogni visita DFS di un grafo, per ogni coppia di vertici $u$, $v$, una ed una sola delle seguenti condizioni è soddisfatta: 
    \begin{itemize}
        \item $d[u]<d[v]<f[v]<f[u]$ ed $u$ è un \textbf{antenato} di $v$ in un albero della foresta DFS 
        \item $d[v]<d[u]<f[u]<f[v]$ ed $u$ è un \textbf{discendente} di $v$ in un albero della foresta DFS
        \item $d[u]<f[u]<d[v]<f[v]$ e tra $u$ e $v$ non esiste relazione (non sono adiacenti)
    \end{itemize}
\end{theorem}
\begin{theorem}[Annidamento degli intervalli]
    Una visita DFS di un grafo colloca un vertice $v$ come discendente proprio di un vertice $u$ in un albero della foresta 
    DFS se e solo se $d[u]<d[v]<f[v]<f[u]$.
\end{theorem}
\begin{theorem}[Cammino bianco]
    In una foresta DFS, un vertice $v$ è discendente del vertice $u$ se e solo se al tempo d[$u$] $v$ è raggiungibile da 
    $u$ con un cammino contenente solo vertici bianchi.
\end{theorem}
La complessità della visita DFS è $\mathcal(O)(m+n)$ (come la visita generica).
\section{Aciclicità}
È possibile verificare la presenza di un ciclo tramite una visita DFS.
\subsection{Grafi orientati}
Un arco $\langle u,v \rangle$ viene \textbf{percorso} quando si incontra $v$ nella lista degli adiacenti ad $u$. Durante 
la DFS di un grafo orientato, ogni arco è percorso una volta sola. Definiamo:
\begin{itemize}
    \item \textbf{Arco dell'albero}: arco inserito nella foresta DFS
    \item \textbf{Arco all'indietro}: arco che collega un vertice ad un suo antenato
    \item \textbf{Arco in avanti}: arco che collega un vertice ad un suo discendente 
    \item \textbf{Arco di attraversamento}: arco che collega due vertici che non sono in relazione 
\end{itemize}
Durante la visita di un grafo orientato, un arco $\langle u,v \rangle$ viene percorso quando si incontra $v$ nella lista 
degli adiacenti ad $u$. In quel momento, color[$v$] può essere:
\begin{itemize}
    \item \textbf{bianco}: $\langle u,v \rangle$ è un \textbf{arco dell'albero}
    \item \textbf{grigio}: $u$ è un discendente di $v$, $\langle u,v \rangle$ è un \textbf{arco all'indietro} 
    \item \textbf{nero}: $\langle u,v \rangle$ è un arco
    \begin{itemize}
        \item \textbf{in avanti} se $v$ è discendente di $u$
        \item \textbf{di attraversamento} altrimenti
    \end{itemize}
\end{itemize}
\subsection{Grafi non orientati}
Durante la DFS di un grafo orientato, ogni arco è percorso esattamente due volte. Definiamo:
\begin{itemize}
    \item \textbf{Arco dell'albero}: arco inserito nella foresta DFS
    \item \textbf{Arco all'indietro}: arco che collega un vertice ad un suo antenato
\end{itemize}
Durante la visita di un grafo orientato, un arco $\langle u,v \rangle$ viene percorso quando si incontra $v$ nella lista 
degli adiacenti ad $u$. In quel momento, color[$v$] può essere:
\begin{itemize}
    \item \textbf{bianco}: $(u,v)$ è un \textbf{arco dell'albero}
    \item \textbf{grigio}: $u$ è un discendente di $v$, $\langle u,v \rangle$ è un \textbf{arco all'indietro} 
    \item \textbf{nero}: $\langle u,v \rangle$ è un \textbf{arco all'indietro}
\end{itemize}
\subsection{Test di aciclicità}
\begin{theorem}[Grafo aciclico]
    Se un grafo (orientato o non orientato) contiene un ciclo, allora esiste un arco all'indietro. Viceversa, se una visita 
    in profondità produce un arco all'indietro, il grafo contiene un ciclo. Quindi, un grafo è aciclico se e solo se una 
    visita DFS non produce archi all'indietro.
\end{theorem} 
\begin{algorithm}[H]
    \caption{CICLICO(G)}
    \begin{algorithmic}
        \State INIZIALIZZA(G)
        \For{ogni nodo $u$ di $G$}
            \If{color[$u$] = white \textbf{and} VISITA RICORSIVA CICLO(G,u)}
                \State \textbf{return} true
            \EndIf 
        \EndFor
        \State \textbf{return} false
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
    \caption{VISITA RICORSIVA CICLO(G,u)}
    \begin{algorithmic}
        \State color[$u$] $\gets$ gray
        \For{ogni $v$ adiacente ad $u$}
            \If{color[$v$ = white]}
                \State $\pi[v] \gets u$
                \If{VISITA RICORSIVA CICLO(G,v)}
                    \State \textbf{return} true
                \EndIf
                \ElsIf{$v\neq \pi[u]$ (color[$v$] = gray \textit{per grafi orientati})}
                    \State \textbf{return} true
            \EndIf 
        \EndFor 
        \State color[$u$] $\gets$ black 
        \State \textbf{return} false
    \end{algorithmic}
\end{algorithm}
\section{Ordinamento topologico}
Dato un \textbf{grafo orientato aciclico} (DAG) è sempre possibile ordinare i nodi in un \textbf{ordine topologico}, cioè 
in modo che non ci sia nessun arco all'indietro nell'ordinamento.
\subsection{Raggiungibilità}
In un DAG, la relazione di \textbf{raggiungibilità} è una relazione di ordine parziale:
\begin{itemize}
    \item è riflessiva: ogni vertice è raggiungibile da se stesso 
    \item è antisimmetrica: se $v$ è raggiungibile da $u$ ed $u$ è raggiugibile da $v$, allora $v$ e $u$ sono coincidenti
    \item è transitiva: se $v$ è raggiungibile da $u$ e $w$ è raggiungibile da $v$, allora $w$ è raggiungibile da $u$
\end{itemize}
\subsection{Ordine topologico}
Dato un DAG, un \textbf{ordine topologico} è un ordine lineare dei suoi nodi tale che, se nel grafo vi è un arco $(u,v)$, 
allora $u$ precede $v$ nell'ordine. Un grafo aciclico possiede sempre un ordine topologico; un DAG può possedere diversi 
ordini topologici.
\subsubsection{Algoritmo astratto}
Il metodo più semplice ma meno efficiente per trovare un'ordinamento topologico. Si definiscono: \textbf{nodo sorgente} 
come nodo che non ha archi entranti, e \textbf{nodo pozzo} come nodo che non ha archi uscenti. Questo algoritmo funziona
rimuovendo nodi sorgente dal grafo $G'$ (memorizzandoli in una lista, $ord$) e sviluppando un sottoproblema.
\begin{theorem}
    $ord$ è un'ordinamento topologico di $G$. Denotiamo con $ord_i$ e $G'$ (copia del grafo $G$) il contenuto di $ord$ e
    $G'$ all'iterazione i-esima: ad ogni istante del ciclo, non può esserci nessun cammino in $G$ che porta da un vertice 
    in $G'$ ad uno in $ord$ ("all'indietro").
\end{theorem}
\begin{proof}
    Caso base: con $i=0$, la condizione è banalmente verificata ($ord$ non contiene vertici).
    Passo induttivo: ad un istante $k$, non c'è alcun cammino in $G$ dai vertici $G'_k$ ai vertici in $ord_k$, quindi 
    ad un istante $k+1$, non c'è alcun cammino in $G$ dai vertici $G'_{k+1}$ ai vertici in $ord_{k+1}$.\\
    Al passo k-esimo, si sceglie un vertice sorgente $u$ in $G'_k$: non ci sono cammini in $G$ che raggiungono $u$ passando solo 
    per i vertici di $G'_k$. Per ipotesi induttiva, aggiungendo $u$ ad $ord_k$ all'istante $k$, all'istante $k+1$ non ci 
    sarà alcun cammino in $G$ dai vertici di $G'_{k+1}$ ai vertici in $ord_{k+1}$.
\end{proof}
La complessità dell'algoritmo semplice è $\mathcal{O}(m*n)$.
\subsubsection{Ordinamento topologico basato su DFS}
\begin{theorem}[Ordinamento topologico]
    In una qualunque visita DFS, $f[v]<f[u]$ per ogni arco $\langle u,v \rangle$.
\end{theorem}
\begin{proof}
    Supponiamo per assurdo che si abbia $f[u]<f[v]$; quindi, apro e chiudo $u$ e poi apro e chiudo $v$, oppure apro $u$ 
    e $v$ e poi chiudo $u$ e $v$. Il primo caso è impossibile, perchè $u$ non può diventare nero prima che $v$ diventi 
    grigio, ossia prima che tutti i suoi adiacenti siano stati scoperti. Il secondo caso è impossibile perchè $u$ sarebbe 
    discendente di $v$ e l'arco $\langle u,v \rangle$ sarebbe un arco all'indietro, ma il grafo è aciclico.
\end{proof}
Per ottenere l'ordinamento topologico, basta tenere traccia dei vertici chiusi con una lista, che conterrà i vertici chiusi, 
dal più al meno recente.
\begin{algorithm}[H]
    \caption{TOPOLOGICAL SORT(G)}
    \begin{algorithmic}
        \State INIZIALIZZA(G)
        \State $ord \gets$ vettore di lunghezza n 
        \State $t \gets$ n-1
        \For{ogni $u \in V$}
            \If{color[$u$] = white}
                \State TOPOLOGICAL DFS(G,u,ord,t)
            \EndIf
        \EndFor\\
        \Return $ord$
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
    \caption{TOPOLOGICAL DFS(G,u,ord,t)}
    \begin{algorithmic}
        \State color[$u$] $\gets$ gray 
        \State d[$u$] $\gets$ time $\gets$ time+1
        \For{ogni $v$ adiacente ad $u$}
            \If{color[$v$] = white}
                \State $\pi gets u$
                \State TOPOLOGICAL DFS(G,v,ord,t)
            \EndIf 
        \EndFor 
        \State color[$u$] $\gets$ black 
        \State f[$u$] $\gets$ time $\gets$ time+1
        \State $ord[t] \gets u$
        \State t--
    \end{algorithmic}
\end{algorithm}
La complessità è $\mathcal{O}(m+n)$.
\section{Componenti connesse e fortemente connesse}
\subsection{Componenti connesse}
In un grafo non orientato, la relazione di raggiungibilità è una relazione di equivalenza:
\begin{itemize}
    \item è \textbf{riflessiva}: per definizione, ogni vertice è raggiungibile da se stesso con un cammino degenere di 
    lunghezza 0
    \item è \textbf{simmetrica}: se $v$ è raggiungibile da $u$ tramite un cammino $p$, allora $u$ è raggiungibile da $v$ 
    percorrendo all'indietro lo stesso cammino
    \item è \textbf{transitiva}: se $v$ è raggiungibile da $u$ e $w$ è raggiungibile da $v$, allora $w$ è raggiungibile 
    da $u$, percorrendo il cammino da $u$ a $v$ e da $v$ a $w$.
\end{itemize}
In un grafo non orientato, le componenti connesse sono le classi di equivalenza della relazione di raggiungibilità. Una 
visita di un grafo restituisce esattamente una componente connessa di quel grafo (l'albero di visita).
\begin{algorithm}[H]
    \caption{COMPONENTI CONNESSE(G)}
    \begin{algorithmic}
        \State INIZIALIZZA(G)
        \State $\Pi \gets$ foresta vuota 
        \For{ogni $u \in V$}
            \If{color[$u$] = white}
                \State $\pi_u \gets$ VISITA(G,u) ($\pi_u$ è l'albero di visita)
                \State $\Pi \gets \Pi+{\pi_u}$
            \EndIf 
        \EndFor\\
        \Return $\Pi$
    \end{algorithmic}
\end{algorithm}
Il grafo è connesso se e solo se nell'albero di visita tutti i vertici hanno un predecessore eccetto il nodo sorgente.
\begin{algorithm}
    \caption{CONNESSIONE(G)}
    \begin{algorithmic}
        \State INIZIALIZZA(G)
        \State scegli vertice $s$ appartenente a $G$
        \State $\pi \gets$ VISITA(G,s)
        \For{ogni $u$ appartenente a $G$}
            \If{$u\neq s$ e $\pi[u]$=NULL}
            \Return false 
            \EndIf
        \EndFor\\ 
        \Return true
    \end{algorithmic}
\end{algorithm}
La complessità è $\mathcal{O}(m+n)$.
\subsection{Componenti fortemente connesse}
In un grafo orientato $G$, due nodi, $u$ e $v$ si dicono mutualmente raggiungibili o \textbf{fortemente connessi} se 
ognuno dei due è raggiungibile dall'altro, cioè esiste un cammino da $u$ a $v$ e da $v$ a $u$ ($u \leftrightarrow v$).
La connessione forte è una relazione di equivalenza.
Una \textbf{cfc} di un grafo orientato $G$ è un sottografo $G'$ di $G$ fortemente connesso e massimale: i nodi di $G'$ sono 
tutti fra loro fortemente connessi e nessun altro nodo di $G$ è fortemente connesso con nodi di $G'$. Ogni vertice del grafo 
fa parte di una cfc, in quanto ogni nodo è almeno fortemente connesso con se stesso. Da un grafo fortemente connesso è possibile 
ricavare un DAG (e quindi un ordinamento topologico). 

Per trovare la cfc contenente un vertice $x$:
\begin{itemize}
    \item calcoliamo i \textbf{discendenti} di $x$ (D($x$)), ossia i vertici di $G$ raggiungibili da $x$
    \item calcoliamo gli \textbf{antenati} di $x$ (A($x$)), ossia i vertici che raggiungono $x$
    \item cfc[$x$] è data dall'\textbf{intersezione} tra l'insieme degli antenati e dei discendenti ($D(x)\cap A(x)$)
\end{itemize}
Questo algoritmo è molto costoso, $\mathcal{O}(n^2+m)$.
\begin{lemma}[Cammino fortemente connesso]
    Se due vertici $x,y$ di un grafo sono in una stessa cfc, allora nessun cammino tra di essi può abbandonare tale cfc.
\end{lemma}
\begin{proof}
    Sia $z$ tale che $x\rightarrow z$ e $z\rightarrow y$. $z$ è banalmente raggiungibile da $x$ per ipotesi ($x\rightarrow z$); 
    siccome $x,y$ appartengono alla stessa cfc, esisterà un cammino $y\rightarrow x$. Esiste anche $z\rightarrow y$ per 
    ipotesi. Quindi, per concatenazione, esisterà anche un cammino $z\rightarrow y\rightarrow x$.
\end{proof}
\begin{theorem}[Sottoalbero fortemente connesso]
    In una qualunque DFS di un grafo $G$ orientato, tutti i vertici di una cfc vengono collocati in uno stesso sottoalbero.
\end{theorem}
\begin{proof}
    Sia $r$ il primo vertice di una data cfc che viene scoperto dalla DFS: da $r$ sono raggiungibili tutti gli altri vertici 
    della cfc (per definizione). Poichè $r$ è il primo vertice ad essere stato scoperto, al momento della sua scoperta tutti 
    gli altri vertici della cfc saranno bianchi. Per il lemma precedente, tutti i cammini da $r$ agli altri vertici della 
    cfc conterranno solo vertici bianchi che fanno parte della cfc. Allora, per il \textbf{teorema del cammino bianco}, 
    tutti i vertici appartenenti alla cfc di $r$ saranno discendenti di $r$ nell'albero DFS.
\end{proof}
\begin{proprietà}[1]
    Esiste sempre, per ogni grafo diretto, almeno un ordine di visita DFS dei suoi nodi tale per cui le cfc sono già 
    separate nella foresta di visita.
\end{proprietà}
\begin{proprietà}[2]
    Un grafo $G$ ed il suo trasposto $G^T$ hanno le stesse cfc.
\end{proprietà}
Siano $x$ e $y$ due vertici di un grafo $G$; assumiamo che $x$ non sia sulla stessa cfc di $y$. Dopo la DFS su $G$ si possono 
presentare i seguenti casi:
\begin{enumerate}
    \item \textbf{$y$ è discendente di $x$} in un albero della foresta DFS di $G$. Esiste un cammino da $x$ a $y$, ma non
    il contrario, quindi non esisterà un cammino da $x$ a $y$ in $G^T$ ($d[x]<d[y]<f[y]<f[x]$).
    \item \textbf{$x$ e $y$ non sono uno discendente dell'altro} nella foresta DFS di $G$. Non può esistere nessun cammino 
    da $y$ a $x$, altrimenti $x$ sarebbe nel sottoalbero di $y$. Quindi, non eeisterà il cammino da $x$ a $y$ in $G^T$ 
    ($d[y]<f[y]<d[x]<f[x]$).
\end{enumerate}
In entrambi i casi, ($f[x]>f[y]$), quindi nella seconda visita i vertici saranno considerati in ordine decrescente di tempo 
di fine visita.

Da queste osservazioni, si ricava l'\textbf{algoritmo di Kosaraju}: 
\begin{enumerate}
    \item visita $G$ con l'algoritmo VISITA TUTTI I VERTICI DFS e costruisci una lista di vertici in ordine decrescente 
    dei tempi di fine visita 
    \item costruisci $G^T$
    \item visita $G^T$ con l'algoritmo VISITA TUTTI I VERTICI DFS, considerando i vertici nell'ordine trovato al passo 1 
\end{enumerate}
La complessità è pari a $\mathcal{O}(n+m)$.
Per dimostrare la correttezza dell'algoritmo, ci si avvale del \textbf{teorema del sottoalbero fortemente connesso} e dei 
lemmi seguenti.
\begin{lemma}[2]
    Un grafo orientato e il suo trasposto hanno le stesse cfc.
\end{lemma}
\begin{lemma}[3]
    Sia $A^T$ un albero ottenuto con la visita in profondità $G^T$, considerando i vertici in ordine decrescente dei tempi 
    di fine visita su $G$, e sia $u$ la sua radice. Per ogni vertice $v$ discendente di $u$ in $A^T$, $v$ e $u$ appartengono 
    alla stessa cfc.
\end{lemma}
\begin{proof}
    Dimostriamo che ogni discendente di $u$ in $A^T$ è anche un discendente di $u$ in un albero della foresta costruita 
    dalla DFS su $G$. La dimostrazione è fatta per assurdo.\\
    Consideriamo un cammino sull'albero $A^T$ a aprtire dalla radice $u$; sia $v$ il primo vertice sul cammino per cui il 
    lemma non vale (cioè $v$ non è discendente di $u$ nella visita di $G$) e sia $w$ il suo predecessore sul cammino. 
    L'enunciato è valido per $w$: $d[u]\leq d[w]<f[w]\leq f[u]$.\\
    Siccome la visita DFS di $G^T$ considera i vertici in ordine decrescente di fine visita, vale $f[v]<f[u]$.
    Per il \textbf{teorema delle parentesi}, se $v$ non è discendente di $u$ nella prima visita, deve valere $d[v]<f[v]<d[u]<f[u]$.
    Ma questo è impossibile, in quanto $v$ è adiacente a $w$ in $G$, e la visita di $v$ non può terminare prima che sia 
    iniziata la visita di un suo adiacente.\\
    Quindi in $G$ esiste un cammino da $u$ a $v$. Siccome $v$ è discendente di $u$ in $A^T$, esiste anche un cammino da 
    $u$ a $v$ in $G^T$, e quindi da $v$ a $u$ in $G$.
\end{proof}
L'algoritmo è quindi corretto.
\section{Tecnica greedy}
\subsection{Problemi}
Un \textbf{problema P} è definito come una relazione $P\subseteq I\times S$: $I$ è l'insieme delle possibili \textbf{istanze}
del problema e l'insieme $S$ è l'insieme delle possibili \textbf{soluzioni} del problema.
Tipi di problemi:
\begin{itemize}
    \item \textbf{problemi di decisione}: problemi che richiedono di verificare una certa proprietà sull'input (risultato 
    o vero o falso)
    \item \textbf{problemi di ricerca}: data un'istanza del problema, restituire una soluzione ammissibile
    \item \textbf{problemi di ottimizzazione}: data un'istanza di un problema ed una funzione obiettivo valuta(Sol), che per 
    ogni soluzione Sol restituisce un valore di tale soluzione Sol ottima
\end{itemize}
Una soluzione di un problema di ottimizzazione è anche una soluzione di un equivalente problema di ricerca (ma non viceversa).
\subsection{Problema dello zaino frazionario}
\subsubsection{Definizione}
Un ladro entra in un magazzino e trova $n$ oggetti. L'i-esimo oggetto $o_i$ ha un valore di $c_i$ euro e pesa $p_i$ chilogrammi.
$v_i=c_i/p_i$ è il valore per unità di peso. Gli oggetti sono frazionabili, quindi il ladro ne può prendere anche solo una 
frazione $x_1$, $0\leq x_i \leq 1$. In tal caso, il valore della parte presa sarà $c_ix_i$. Il ladro ha un solo zaino, che 
può contenere oggetti per un peso massimo di $P$ chilogrammi.

Quali oggetti e in quale quantità dovrà prendere il ladro per ottenere il massimo guadagno dal furto?
\subsubsection{Formalizzazione}
Il problema richiede di valorizzare $x_i$ per ogni $1\leq i \leq n$ in modo che $\sum x_ic_i$ sia massima. Due proprietà: 
\begin{itemize}
    \item \textbf{Ottimalità}. Tutte le soluzioni che massimizzano il valore totale dello zaino (la \textbf{funzione obiettivo}) 
    sono soluzioni ottime. 
    \item \textbf{Ammissibilità}. Tutte le valorizzazioni dei vari $x_i$ tali che il peso sia minore di $P$ e che $x_i$ 
    sia compreso tra 0 e 1 sono soluzioni ammissibili.
\end{itemize}
\subsection{Sottostruttura ottima e scelta greedy}
Un problema ha la proprietà della \textbf{sottostruttura ottima} se una soluzione ottima del problema include le soluzioni 
ottime dei suoi sottoproblemi.\\
Se un problema di ottimizzazione gode delle proprietà della sottostruttura ottima, in genere può essere risolto con tecniche 
di tipo \textbf{greedy} i di \textbf{programmazione dinamica}.

Un problema gode della proprietà della \textbf{scelta greedy} se è sempre possibile scegliere in esso una variabile decisionale, 
attribuirle un valore ammissibile e localmente ottimo, toglierla dal problema ed ottenere un sottoproblema la cui soluzione, 
unita al valore di questa variabile, sia una soluzione ottima per il problema originario.

Viene definito un \textbf{criterio di appetibilità}, che permette di effettuare efficientemente la scelta greedy. L'appetibilità 
ordina le variabili in maniera che in ogni sottoproblema sia possibile effettuare efficientemente la scelta greedy.
\subsection{Schema di algoritmo greedy}
Passi di un algoritmo greedy:
\begin{enumerate}
    \item scegli la variabile del problema con appetibilità maggiore 
    \item attribuisci ad essa il valore ammissibile più promettente a livello locale 
    \item ottieni il sottoproblema risultante dall'eliminazione della variabile al punto 1
    \item se il problema è risolto, restituisci i valori delle variabili assegnati al punto 2, altrimenti vai al punto 1 
\end{enumerate}
Nella maggior parte dei casi, gli algoritmi greedy affrontano problemi dove, dato un insieme di elementi $A$, è necessario 
selezionare un sottoinsieme $S$ di elementi ammissibile e ottimo.
\begin{algorithm}
        \caption{GREEDY(A)}
    \begin{algorithmic}
        \State $S \gets$ insieme vuoto
        \While{$S$ non è una soluzione}
            \State estrai da $A$ l'elemento più appetibile $a$
            \If{$S\cup {a}$ è ammissibile}
                \State aggiungi $a$ all'insieme $S$
            \EndIf 
        \EndWhile
    \end{algorithmic}
\end{algorithm}
Le appetibilità possono essere fisse o modificabili.
\subsection{Massimo numero di intervalli disgiunti}
\subsubsection{Problema}
Dato un insieme di intervalli, trovare il massimo numero di intervalli disgiunti significa trovare un sottoinsieme costituito 
da intervalli tutti disgiunti, tale che il numero di intervalli sia massimo.

La soluzione ottimale si ottiene scandendo l'insieme di intervalli scegliendo ogni volta l'intervallo che finisce prima:
\begin{enumerate}
    \item orina l'insieme degli intervalli in una sequenza $S$ ordinata secondo l'istante finale
    \item inizializza la soluzione Sol come sequenza vuota 
    \item scandisci $S$ in ordine, e per ogni suo elemento $A$:
    \begin{enumerate}
        \item se $A$ inizia dopo la fine dell'ultimo elemento di Sol, aggiungilo in fondo a Sol 
        \item altrimenti, non aggiungerlo 
    \end{enumerate}
\end{enumerate}
La complessità è quella dell'ordinamento del passo 1 più quella della scansione di $S$ al passo 3, $\mathcal{O}(n\log n)$.
\subsubsection{Correttezza}
Definiamo le invarianti ($S$ è l'insieme di intervalli esaminati ad un passo intermedio $k$): 
\begin{itemize}
    \item \textbf{Max}: la sequenza $A_1,A_2,\dots,A_k$ di intervalli disgiunti scelti è, per l'insieme $S$, una \textbf{sequenza massimale}
    \item \textbf{PrimaMax}: la sequenza $A_1,A_2,\dots,A_k$ è, fra le sequenze massimali, quella che finisce prima 
    \item \textbf{PrimaVisti}: ogni intervallo $\in S$ termina prima della fine di qualunque intervallo $\notin S$
\end{itemize}
L'invariante da dimostrare è \textbf{Max}.
\begin{proof}
    \textbf{Caso base}. Inizialmente, $S$ è l'insieme vuoto, la sequenza massimale per $S$ è la sequenza vuota. Tutte e 
    tre le invarianti sono vere.\\
    \textbf{Passo}. Sia $A$ l'intervallo che termina prima tra quelli ancora da esaminare (ossia fra quelli $\notin S$).
    Consideriamo allora l'insieme $S'\equiv S \cup \{A\}$ e cerchiamo di stabilire qual è per $S'$ la sequenza massimale 
    (\textbf{Max}) che finisce prima (\textbf{PrimaMax}).\\
    \textbf{Caso 1}: $A$ inizia \textbf{prima} della fine della sequenza $A_1,A_2,\dots,A_k$, quindi $A$ interseca $A_k$; quindi, 
    ogni sequenza avente $A$ come ultimo elemento non può avere più di $k$ elementi. Non inserendo $A$ nella soluzione, 
    l'invariante Max si mantiene.\\
    \textbf{Caso 2}: $A$ inizia \textbf{dopo} la fine della sequenza $A_1,A_2,\dots,A_k$; allora, la sequenza $A_1,A_2,\dots,A_k,A$: 
    \begin{itemize}
        \item è per $S'$ una sequenza massimale, perchè una sequenza massimale per $S'$ non può avere più di $k+1$ elementi 
        \item è per $S'$ la sequenza massimale che finisce prima, perchè in $S'$ non ci sono sequenze massimali non includenti 
        $A$
    \end{itemize}
    Anche in questo caso, le invarianti si mantengono. 
\end{proof}
\subsection{Algoritmo di Moore}
\subsubsection{Problema dello scheduling e algoritmo risolvente}
Una \textbf{sequenza di shceduling}, o semplicemente \textbf{scheduling}, è una sequenza di lavori $L_1,L_2,\dots,L_n$ che 
devono essere eseguiti uno dopo l'altro consecutivamente. L'algoritmo è greedy, l'appetibilità è data dalla scadenza. 
Descrizione informale:
\begin{enumerate}
    \item ordina la sequenza dei lavori per ordine crescente di istante di scadenza 
    \item scandisci tale sequenza in ordine, aggiungendo ogni volta il successivo lavoro alla fine della sequenza di scheduling 
    provvisoria; se così facendo il lavoro considerato termina dopo la scadenza, si elimina dalla sequenza di scheduling 
    il lavoro di durata massima 
\end{enumerate}
\begin{algorithm}
    \caption{MOORE()}
    \begin{algorithmic}
        \State $Sol \gets$ sequenza vuota
        \For{i=n to n}
        \State aggiungi $L_i$ a $Sol$
        \State $t=t+$durata di $L_i$
            \If{$t>$scadenza di $L_i$}
                \State togli da $Sol$ il lavoro $L_{max}$ di durata massima 
                \State $t=t-$durata di $L_{max}$
            \EndIf 
        \EndFor
    \end{algorithmic}
\end{algorithm}
\subsubsection{Correttezza}
L'algoritmo di Moore è corretto, si dimostra per induzione.
\begin{proof}
    \textbf{Invariante}. Sia $S$ l'insieme di tutti i job finora esaminati:
    \begin{enumerate}
        \item $Sol$ è uno \textbf{scheduling massimale} $L_1,L_2,\dots,L_k$ di job di $S$ che rispetta le scadenze, cioè 
        quello con il numero massimo di elementi 
        \item $Sol$ è, fra tutti gli scheduling massimali di job di $S$, quello di durata totale minima 
        \item $Sol$ è ordinato per tempi di scadenza crescenti
        \item ogni job non in $S$ ha una scadenza posteriore o uguale alle scadenze dei job in $S$ 
    \end{enumerate}
    \textbf{Passo}. Sia $t_k$ l'istante di fine dello scheduling $Sol=L_1,L_2,\dots,L_k$. Sia $L$ il primo job non ancora 
    esaminato, cioè non in $S$, che ha scadenza prima di tutti gli altri job non esaminati. Siano $s$ la scadenza di $L$ 
    e $d$ la durata di $L$. Considerando l'insieme $S'=S\cup \{L\}$, si possono verificare due casi:
    \begin{enumerate}
        \item $L$ aggiunto ad $S$ è eseguibile entro la sua scadenza 
        \item $L$ non è eseguibile
    \end{enumerate}
    \textbf{Caso 1}. Lo scheduling così ottenuto è uno scheduling massimale. $\{L\}$ ha cardinalità 1; quindi, se esistesse 
    uno scheduling per $S\cup \{L\}$ con più di $k+1$ elementi, vi sarebbe uno scheduling per $S$ con più di $k$ elementi,
    il che è assurdo. Inoltre $S\cup \{L\}$ è di durata minima.\\ 
    \textbf{Caso 2}. Lo scheduling $L_1,L_2,\dots,L_k,L$ non è una soluzione. Se $L$ è il processo di durata massima, sostituendolo 
    ad un job già presente $S$, si otterrebbe uno scheduling di durata maggiore o uguale (violando punto 2). Se in $S$ 
    esiste un job $L_{max}$ di durata maggiore di $L$, eliminando $L_{max}$ ed aggiungendo $L$ ad $S$, si ottiene uno 
    scheduling di ancora $k$ elementi, di durata minore (minima).\\
    Quindi, $L_1,L_2,\dots,L_k$ è uno scheduling massimale e di durata minima.
\end{proof}
\subsection{Codici di Huffman}
\subsubsection{Codifica}
Una \textbf{codifica} associa un insieme di caratteri o simboli ad un insieme di altri elementi, detti \textbf{parole in codice}.
Le codifiche possono avere lunghezza fissa o variabile. La codifica deve essere non ambigua, cioè che nessuna codifica di 
di un carattere sia prefisso di un'altra. Un codice binario prefisso può essere rappresentato tramite un albero binario 
in cui le foglie rappresentano i caratteri ed i cammini dalla radice alle foglie rappresentano la codifica dei caratteri. 
L'albero prende il nome di \textbf{albero di codifica}.
\subsubsection{Codifica ottima}
Il problema affrontato dall'algoritmo di Huffman è il seguente: dato un testo scritto secondo un certo alfabeto $C$, trovare 
una codifica che sia minimale, cioè renda minima la lunghezza del testo codificato (è una tecnica di compressione). 

Una codifica a lunghezza fissa usa parole in codice tutte della stessa dimensione. Con questa codifica servono almeno 
$\lceil \log_2 n\rceil$ bit per rappresentare ogni parola in codice per un alfabeto di $n$ elementi.

Un albero avente nodi interni con un solo figlio non è ottimale: può essere eliminato attaccando i suoi figli direttamente 
al padre. Un albero binario in cui ogni nodo interno ha esattamente due figli si chiama \textbf{albero pieno}.

Per ottenere la maggior compressione possibile, i caratteri più frequenti devono avere le codifiche più corte, cioè comparire 
a livelli più alti dell'albero.
\subsubsection{Algoritmo di Huffman}
L'algoritmo di Huffman ha come input: un alfabeto, cioè un insieme $C$ di caratteri; una funzione $f$ che da' la frequenza 
di ciascun carattere in un dato testo $t$.
Produce in output un codice binario ottimo per la compressione di quel testo $t$.
Con $f(c)=$ frequenza con cui il carattere $c$ compare nel testo e $d_c=$ livello del carattere $c$ nell'albero $T$, si 
chiama lunghezza media di codifica o costo di $T$:
\begin{equation}*
    L(T)=\Sigma_{c\in C}d_C\cdot f(c)
\end{equation}
Se n è il numero di caratteri che compongono il testo con frequenze date dalla funzione $f$, la lunghezza in bit della 
codifica del testo è data da:
\begin{equation}*
    B(T)=\Sigma_{c\in C}d_C\cdot n\cdot f(c)=n\cdot L(T)
\end{equation}

L'algoritmo di Huffman è un'applicazione della tecnica greedy \textbf{con appetibilità modificabili}:
\begin{enumerate}
    \item Per ciascun carattere, crea un albero formato solo da una foglia contenente il carattere e la frequenza del carattere 
    \item Fondi i due alberi che hanno due frequenze minime e costruisci un nuovo albero che ha come frequenza la somma 
    delle frequenze degli alberi fusi 
    \item Ripeti la fusione finchè si ottiene un unico albero 
\end{enumerate}
\begin{algorithm}
    \caption{HUFFMAN(C,f)}
    \begin{algorithmic}
        \State $n\gets |C|$ (insieme di caratteri)
        \State $Q\gets$ coda di priorità (heap) vuota
        \For{ogni carattere in $C$}
            \State enqueue(Q,createTreeNode(c,NULL,NULL),f[c])
        \EndFor 
        \For{$i=0;i<n-1;i++$}
            \State $x\gets$ dequeueMin(Q)
            \State $y \gets$ dequeueMin(Q)
            \State $z \gets$ createTreeNode(null,x,y)
            \State $f[z] \gets f[x]+f[y]$
            \State enqueue(Q,z,f[z])
        \EndFor\\
        \Return dequeueMin(Q)
    \end{algorithmic}
\end{algorithm}
La complessità dell'algoritmo è $\mathcal{O}(\log n)$ se la coda di priorità è realizzata con un heap.
\subsubsection{Correttezza}
L'algoritmo restituisce un albero di Huffman, che rende minima la lunghezza media di codifica.

\textbf{Foresta di Huffman}. La foresta di Huffman per un alfabeto $C$ con funzione di frequenza $f$ è una foresta i 
cui elementi $T_1,T_2,\dots,T_n$ sono sottoalberi di un albero di Huffman $T$ per quell'alfabeto.

\textbf{Invariante}. La foresta $\{T_1,T_2,\dots,T_n\}$ costruita dall'algoritmo al passo generico è una foresta di 
Huffman per $C$ e $f$, cioè esiste un albero di Huffman $T$ di cui gli alberi $T_1,T_2,\dots,T_n$ sono sottoalberi.

\textbf{Caso base}. Prima dell'esecuzione del ciclo, l'invariante vale banalmente, in quanto gli alberi sono tutti 
nodi singoli, cioè foglie corrispondenti ai caratteri dell'alfabeto $C$.

\textbf{Passo}. Assumiamo che prima della k-esima iterazione del ciclo l'invariante valga, cioè che la foresta 
$F=\{T_1,T_2,\dots,T_n\}$ sia una foresta di Huffman per il dato alfabeto $C$. Mostriamo che dopo il (k+1)-esimo passo 
dell'iterazione, che fonde i due alberi $T_a$ e $T_b$ aventi due frequenze minime in un nuovo albero $T_{ab}$, la nuova 
foresta $F-\{T_a,T_b\}\cup\{T_{ab}\}$ è ancora una foresta di Huffman.
\begin{proof}
    Mostriamo che fra tutti gli alberi di codifica di cui $T_1,T_2,\dots,T_n$ sono sottoalberi, vi è un albero $(T''')$ 
    la cui lunghezza media di codifica $L(T''')$ è minima e di cui $T_{ab}$ è un sottoalbero. Quindi l'albero $T_{ab}$ può 
    essere inserito nella foresta al posto di $T_a$ e $T_b$: la foresta risultante $F-\{T_a,T_b\}$ è ancora una foresta 
    di Huffman.\\
    Per ipotesi induttiva, esiste un albero $T'$ di cui gli alberi $T_1,\dots,T_a,\dots,T_b\dots,T_n$ sono sottoalberi, 
    dove $T_a$ e $T_b$ sono, nella foresta al passo considerato, i due alberi di frequenze minime, $f(T_a)$ e $f(T_b)$. 
    Mostriamo che esiste un albero di Huffman $T'''$ avente $T_{ab}$ come sottoalbero.\\
    Consideriamo, fra i nodi interni di $T'$ non appartenenti alla foresta $F$, cioè fra i nodi che nel passo considerato 
    non sono ancora stati creati, quello di profondità massima. Sia esso $z$. Come ogni nodo interno, $z$ deve avere due 
    sottoalberi figli non nulli, siano $T_x$ e $T_y$.\\
    Poichè $T_a$ e $T_b$ sono, al passo considerato, i due alberi di pesi minimi, assumendo $f(T_a)<=f(T_b)$ e $f(T_x)<=f(T_y)$,
    abbiamo $f(T_a)<=f(T_x)$ e $f(T_b)<=f(T_y)$. Poichè $z$ è un nodo di profondità massima, le radici degli alberi $T_x$ 
    e $T_y$ si trovano in $T'$ a profondità $d$ non inferiore a quelle di $T_a$ e $T_b$, siano $d_1$ e $d_2$: quindi, 
    $d_1<=d$ e $d_2<=d$.\\
    Scambiamo di posizione $T_a$ e $T_x$ per ottenere un albero di lunghezza media non superiore (analogamente per $T_b$
    e $T_y$).\\
    Ma $T'$ è un albero di Huffman, cioè avente $L(T')$ minimo. Quindi deve essere $L(T''')=L(T')$, e anche $T'''$, che 
    ha $T_{ab}$ come sottoalbero, è un albero di Huffman. Dunque, la foresta $F-\{T_a,T_b\}\cup\{T_{ab}\}$ è ancora una 
    foresta di Huffman.
\end{proof}
\section{Tecnica Greedy applicata ai grafi}
Il teorema della distanza nell'albero BFS visto per i grafi non pesati non vale per un grafo pesato, quindi $d[v]\neq\delta(s,v)$.
\subsection{Algoritmo di Dijkstra}
L'algoritmo di Dijkstra è una visita BFS in cui la frangia è gestita come in un algoritmo greedy con appetibilità modificabili.
L'appetibilità di un vertice $u$ è data da una stima della distanza tra $s$ e $u$ che l'algoritmo ha in un determinato istante.
Inizialmente, tutti i vertici sono stimati a distanza $\infty$ da $s$, tranne $s$ stesso che ha distanza $d[s]=0$ da se 
stesso. Ad ogni ciclo, scelgo un vertice $u$ da aggiungere all'albero tra quelli non ancora inseriti ma aggiungi dalla 
ricerca, e scelgo quello con \textbf{distanza da $s$ stimata minima}.
\begin{algorithm}[H]
    \caption{DIJKSTRA(G,W,s)}
    \begin{algorithmic}
        \State INIZIALIZZA(G)
        \State $color[s]\gets$ gray
        \State $d[s]\gets 0$
        \While{esistono vertici grigi}
            \State $u\gets$ vertice grigio con $d[u]$ minore
            \State $S\gets S\cup\{u\}$
            \For{ogni $v$ adiacente ad $u$}
                \If{$color[v]\neq$black}
                    \State $color[v]\gets$ gray 
                    \If{$d[v]>d[u]+W(u,v)$}
                        \State $\pi[v]\gets u$
                        \State $d[v]\gets d[u]+W(u,v)$
                    \EndIf
                \EndIf 
            \EndFor 
    \algstore{18}   
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[t]
    \begin{algorithmic}
        \algrestore{18}
            \State $color[u]\gets$ black
        \EndWhile
    \end{algorithmic}
\end{algorithm}
\subsubsection{Miglioramenti}
I nodi grigi sono gestiti con una \textbf{coda di priorità}. È possibile non distiguere nodi bianchi e grigi ed inserire 
tutti nella coda fin dall'inizio, assegnando loro distanza infinita da $s$: i nodi neri sono detti \textbf{definitivi} e 
quelli bianchi e grigi \textbf{non definitivi}. Inoltre, non è nemmeno necessario distinguere i nodi neri, poichè non saranno 
in coda.
\begin{algorithm}[H]
    \caption{DIJKSTRA CON PRIORITY QUEUE(G,W,s)}
    \begin{algorithmic}
        \State INIZIALIZZA(G)
        \State $D\gets$ coda di priorità vuota
        \State $d[s]\gets 0$
        \For{ogni $v$ in $v[G]$}
            \State enqueue(D,v,d[v])
        \EndFor
        \While{NotEmpty(D)}
            \State $u\gets$ dequeueMin(D)
            \State $S\gets S\cup\{u\}$
            \For{ogni $v$ adiacente ad $u$}
                \If{$d[v]>d[u]+W(u,v)$}
                    \State $\pi[v]\gets u$
                    \State $d[v]\gets d[u]+W(u,v)$
                    \State decreaseKey(D,v,d[v])
                \EndIf
            \EndFor
        \EndWhile
    \end{algorithmic}
\end{algorithm}
\subsubsection{Complessità}
Denotiamo con:
\begin{itemize}
    \item $t_c$: tempo impiegato dalla costruzione della coda 
    \item $t_e$: tempo impiegato da una estrazione del minimo 
    \item $t_d$: tempo impiegato da una decreaseKey
\end{itemize}
Ad ogni ciclo della visita bisogna estrarre il minimo dalla coda; per ogni arco trovato potrebbe essere necessario decrementare 
la chiave di un vertice. La complessità è quindi: $\mathcal{O}(t_c+n*t_e+m*t_d)$.

La complessità totale dipende anche dalle complessità delle operazioni sulla coda:
\begin{itemize}
    \item con coda di priorità realizzata come sequenza \textbf{non ordinata}: $\mathcal{O}(n^2+m)$
    \item con coda di priorità realizzata come sequenza \textbf{ordinata}: $\mathcal{O}(n+n*m)$
\end{itemize}
\subsubsection{Versione di Johnson}
La versione dell'algoritmo di Dijkstra con coda di priorità implementata come \textbf{heap} è chiamata algoritmo di Johnson.
La complessità è $\mathcal{O}((m+n)\log n)$.
\subsubsection{Correttezza}
\begin{proprietà}[1]
    Un sottocammino di un cammino minimo è un \textbf{cammino minimo}.
\end{proprietà}
\begin{proprietà}[2]
    Siano $S$ l'insieme di vertici già considerati dalla visita e $D$ l'insieme dei vertici ancora da considerare:
    \begin{itemize}
        \item[2.1.] il $d[v]$ di ogni vertice in $S$ non viene più modificato
        \item[2.2.] tutti i predecessori dei nodi nella coda di priorità sono in $S$
        \item[2.3.] per ogni nodo (eccetto $s$), $d[v]\neq \infty$ se e solo se il predecessore di quel nodo non è nullo 
        \item[2.4.] per ogni nodo (eccetto $s$), $d[v]\neq \infty$ implica $d[v]=d[\pi[v]]+W(\pi[v],v)$
    \end{itemize}
\end{proprietà}
\begin{proprietà}[3]
    Se un cammino ha distanza diversa da $\infty$, esiste un cammino tra due nodi nel grafo.
    \begin{proof}
        Per ipotesi induttiva, esiste un cammino da $s$ a $\pi[u]$. Allora, il cammino da $s$ a $\pi[u]$ più l'arco 
        $(\pi[u],u)$ costituisce un cammino da $s$ a $u$.\\
        Supponiamo, per assurdo, che tra $s$ e $u$ vi sia almeno un cammino $s\equiv v_1\rightarrow v_2 \rightarrow \dots \rightarrow v_{k-1} \rightarrow v_k \equiv u$
        e che $u$ venga estratto dalla coda con $d[u]=\infty$. $v_{k-1}$ è già stato estratto quindi 
        $d[u]=d[v_k]=d[v_{k-1}]+W(v_{k-1},v_k)$. Ma $W(v_{k-1},v-k)<\infty$ e $d[u]=\infty$, quindi $d[v_{k-1}]=\infty$.
        Questo può essere iterato per ciascun vertice del cammino, contraddicendo $d[s]=0$.
    \end{proof}
\end{proprietà}
Il predicato $\forall t \in S : d[t]=\delta(s,t)$ è un'invariante del ciclo while.
\begin{proof}
    \textbf{Caso base}. Il predicato è vero perchè all'inizion $S$ è vuoto.

    \textbf{Passo}. Dimostriamo che per il nuovo vertice $u$ estratto da $D$, $d[u]=\delta(s,u)$.

    \textbf{Caso 1}. Il vertice estratto da $D$ ha $d[u]\neq \infty$. Sia $\pi[u]=r\neq NULL$ per proprietà 2.3. Sappiamo 
    allora che $r$ è nell'albero dei cammini minimi e che $d[u]=d[r]+W(r,u)$ (per proprietà 2.4).\\
    Supponiamo, per assurdo, che tra $s$ e $u$ esista un cammino di peso minore di $d[u]$: esso deve contenere un arco che 
    tra un vertice in $s$ e uno in $D$, poniamo siano $x$ il vertice in $S$ e $y$ quello in $D$. Questo cammino può essere 
    visto come la concatenazione di tre cammini ($s\leadsto x\leadsto y \leadsto u$). Se $s\leadsto x\leadsto y \leadsto u$ 
    è minimo, anche $s\leadsto x\leadsto y$ è minimo, quindi $d[y]=\delta(s,y)$. Si ottiene quindi $d[y]+W(y\leadsto u)\geq d[y]\geq d[u]$,
    perchè $u$ è stato estratto e ha $d[u]$ minimo.\\
    Quindi $W(s,\dots x,\dots y,\dots u)=d[y]+W(y\leadsto u)$ non può essere minore di $d[u]$.

    \textbf{Caso 2}. Se $u$ è estratto con $d[u]=\infty$, allora (proprietà 3) non esiste alcun cammino tra $s$ e $u$.
\end{proof} 
\subsubsection{Ulteriori ottimizzazioni}
L'algoritmo si può ottimizzare nel caso serva solamente trovare un cammino minimo tra due nodi, $s$ e $t$:
\begin{itemize}
    \item Ogni volta che si estrae un nodo $u$ da $D$, si può controllare se \textbf{coincide con $t$} e terminare l'algoritmo 
    \item Poichè l'algoritmo esegue una BFS, si vanno a calcolare prima tutti i cammini di lunghezza inferiore a $\delta(s,u)$:
    si possono usare le \textbf{euristiche} per individuare i nodi più promettenti.
\end{itemize}
Dati $s$ e $t$, si può eseguire l'algoritmo contemporaneamente sul grafo $G$ a partire da $s$ e sul grafo trasposto $G^T$ 
a partire da $t$, alternando i passi. Quando si ottengono un cammino minimo da $s$ ed uno da $t$ ad uno stesso nodo $v$,
questo cammino è un cammino da $s$ a $t$ in $G$.

Teniamo in memoria la lunghezza del migior cammino così trovato, inizialmente $d=\infty$. Ogni volta che (in avanti) si percorre 
un arco $(u,v)$ con nodo $u$ nero (in avanti) e nodo v nero (in indietro), se $d[u]+W(u,v)+d^T[v]<d$ si aggiorna $d$.
Quando si estraggono un nodo $x$ (in avanti) e un nodo $y$ (in indietro) tali che $\delta(s,x)+\delta(y,t)>d$, allora il 
cammino trovato di peso $d$ è il cammino minimo.
\subsection{Minimo albero ricoprente}
\subsubsection{Definizione}
Dato un grafo $G$ non orientato e connesso, un albero ricoprente è un \textbf{sottografo} $T\subseteq G$ tale che $T$ è 
un sottografo connesso aciclico e $T$ contiene tutti i vertici di $G$.

Dato un grafo $G$ connesso, non orientato e pesato, il \textbf{minimo albero ricoprente} (MAR) per $G$ è un albero ricoprente 
in cui la somma dei pesi degli archi nell'albero è minima.
\subsubsection{Lemma del taglio}
Un \textbf{taglio} è una partizione dell'insieme $V$ di tutti i nodi del grafo in due parti non vuote, $S$ e $V-S$. Si dice 
che un arco $(u,v)$ \textbf{attraversa il taglio} se i suoi estremi $u$ e $v$ appartengono uno ad una parte ed uno all'altra.

Sia $A$ un insieme di archi appartenenti ad un MAR di un grafo $G$. Consideriamo un taglio non attraversato da alcun arco 
di $A$; siano $S$ e $V-S$ le sue due parti. Sia $(u,v)$ l'arco di peso minimo fra tutti gli archi del grafo che attraversano 
il taglio (chiamato \textbf{arco leggero}): allora $(u,v)$ appartiene a un MAR che estende $A$, cioè l'insieme $A\cup \{(u,v)\}$
è anch'esso un sottoinsieme di un MAR del grafo $G$. 

\begin{proprietà}[1]
    Se in un albero libero si elimina un arco, si ottengono due alberi.
\end{proprietà}
\begin{proprietà}[2]
    Se si connettono due alberi, si ottiene un albero.
\end{proprietà}
\begin{proof}
    Dato un grafo $G$, siano:
    \begin{itemize}
        \item $A$: insieme di archi di $G$ che supponiamo siano appartenenti ad uno stesso MAR di $G$
        \item $(S,V-S)$: un taglio che non taglia nessun arco di $A$ (ma taglia archi di $G$)
        \item $(u,v)$: arco di peso minimo fra quelli di $G$ tagliati dal taglio $(S,V-S)$
    \end{itemize}
    Un MAR di $G$ che estende $A$ è per definizione un albero di peso minimo fra tutti gli alberi ricoprenti  di $G$ che 
    contengono $A$. Un albero ricoprente $AR$ contenente $A$ deve connettere tutti i nodi di $G$, quindi deve contenere 
    un cammino tra $u$ e $v$. Poichè $u$ e $v$ si trovano da parti opposte del taglio, un tale cammino deve contenere almeno 
    un arco $(x,y)$ che attraversa il taglio (possono anche coincidere). Se nell'albero $AR$ sostituiamo l'arco $(x,y)$
    con l'arco $(u,v)$, si ottiene ancora un albero ricoprente di peso totale minore o uguale al peso di $AR$. 
\end{proof}
Poichè l'arco $(u,v)$ non appartiene ad $A$, l'insieme $A\cup \{(u,v)\}$ contiene un arco in più rispetto ad $A$. Aggiungendo 
un arco alla volta si può quindi costruire un MAR ricoprente $G$.
\begin{algorithm}
    \caption{TROVA MAR(G)}
    \begin{algorithmic}
        \State $A\gets$ insieme di archi vuoto
        \While{$A$ non contiene tutti i vertici di $G$}
            \State trova un taglio non attraversato da alcun arco di $A$
            \State aggiungi ad $A$ un arco di peso minimo fra quelli che attraversano il taglio 
        \EndWhile
    \end{algorithmic}
\end{algorithm}
\begin{theorem}[Unicità del MAR]
    Se i pesi degli archi sono tutti distinti, il MAR è unico.
\end{theorem}
\begin{proof}
    Per assurdo, supponiamo che $G$ abbia due minimi alberi ricoprenti distinti, $M1$ e $M2$. Poichè sono distinti, esiste 
    almeno un arco in uno dei due alberi che non appartiene anche all'altro. Sia $e$ l'arco di peso minimo che appartiene 
    solo ad uno dei due MAR (in questo caso, $M1$). Se si aggiunge $e$ ad $M2$, si crea un ciclo $C$. Poichè $M1$ non 
    contiene cicli, nel ciclo c'è almeno un arco $e'$ che non appartiene a $M1$: questo arco ha peso maggiore di $e$ in 
    quanto abbiamo scelto $e$ come arco di peso minimo che appartiene as uno dei due alberi ma non all'altro.\\
    Togliendo $e'$ dal ciclo, si ottiene un albero $M'2$ diverso da $M2$; $M'2$ ha un peso minore di $M2$, fatto che contraddice 
    l'ipotesi iniziale.
\end{proof}
\subsection{Algoritmo di Prim}
Nei MAR si cerca non il nodo più vicino alla radice, ma il nodo più vicino all'albero già costruito; l'appetibilità è 
rappresentata dalla stima della distanza del nodo dall'albero di visita.

Non è possibile eliminare la gestione dei nodi neri in quanto bisogna controllare che tra gli adiacenti del nodo $u$ estratto 
dalla coda non vi sia il genitore di $u$.
\subsubsection{Confronto con l'algoritmo di Dijkstra}
Nell'algoritmo di Prim, a differenza di quello di Dijkstra, $d[v]$ è il peso dell'arco minimo tra $v$ e l'albero di visita 
già costruito nel momento in cui $v$ viene estratto, ma questo non impedisce che in un secondo momento possa esistere un 
arco da un vertice chiuso a $v$ di peso minore di $d[v]$.
L'aggiornamento dell'appetibilità è simile a quello di Dijkstra, ma $d[v]\leftarrow W(u,v)$.
\begin{algorithm}
    \caption{PRIM(G,W,s)}
    \begin{algorithmic}
        \State INIZIALIZZA(G)
        \State $D\gets$ coda di priorità vuota 
        \State $d[s]\gets 0$
        \For{ogni $v$ in $V[G]$}
            \State enqueue(D,v,d[v])
            \State $def[v]\gets$ false
        \EndFor
        \While{NotEmpty(D)}
            \State $u\gets$ dequeueMin
            \State $S\gets S\cup \{u\}$
            \State $def[v]\gets$ true 
            \For{ogni $v$ adiacente ad $u$}
                \If{$def[v]=$false and $d[v]>W(u,v)$}
                \State $\pi[v]\gets u$
                \State $d[v]\gets W(u,v)$
                \State decreaseKey(D,v,d[v])
                \EndIf 
            \EndFor 
        \EndWhile
    \end{algorithmic}
\end{algorithm}
\subsubsection{Complessità}
La complessità dell'algoritmo di Prim è la stessa di quella dell'algoritmo di Johnson, quindi $\mathcal{O}((m+n)\log n)$. 
Siccome il grafo è connesso, $m\geq n-1$, quindi $\mathcal{O}(m\log n)$.
\subsubsection{Correttezza}
Sia $S$ l'albero di visita costruito. $S$ conterrà i nodi definitivi (quelli nell'albero) e $V-S$ contiene i nodi non 
definitivi (non nell'albero). 

Invarianti:
\begin{itemize}
    \item \textbf{IS}: tutti gli archi dell'albero $S$ appartengono ad un qualche minimo albero ricoprente dell'intero 
    grafo $G$
    \item \textbf{ID}: per ogni nodo $x$ non definitivo, $d[x]$ è il peso dell'arco più leggero che collega $x$ ad un nodo 
    nero 
\end{itemize}

\begin{proof}
    \textbf{Caso base}. Dopo la prima iterazione, c'è solo un nodo definitivo, $s$; l'albero $S$ non contiene nessun arco; 
    ogni nodo $x$ adiacente a $s$ ha distanza $d[x]$ uguale al peso dell'arco $(s,x)$ e ogni altro nodo ha distanza uguale 
    a $\infty$. Quindi, \textbf{IS} e \textbf{ID} sono soddisfatti.

    \textbf{Passo}. Scegliamo un taglio che separi i nodi neri dagli altri: sicuramente non taglia nessun arco di $S$.
    Scegliamo $u$ con $d[u]$ minore tra i nodi non neri. $S$ è sottoinsieme di un MAR, e per il lemma del taglio $(y,u)$ 
    appartiene ad un MAR di $G$ che estende $S$ (IS si mantiene). Per ripristinare ID, basta controllare se il nuovo nodo 
    $u$ avvicina qualche suo adiacente ad $S$.
\end{proof}
\subsection{Algoritmo di Kruskal}
\subsubsection{Introduzione}
Considerando la definizione di MAR e la struttura generica di un algoritmo greedy, è possibile definire un algoritmo che 
costruisce iterativamente un MAR, a partire dalla foresta vuota alla quale si aggiungono via via degli archi con i seguenti 
criteri:
\begin{itemize}
    \item \textbf{Appetibilità}: l'appetibilità di un arco è inversamente proporzionale al suo peso 
    \item \textbf{Criterio di ammissibilità}: posso aggiungere un arco alla soluzione provvisoria solo se non crea cicli 
    \item \textbf{Criterio per riconoscere la soluzione}: l'albero costruito è una soluzione solo se è connesso 
\end{itemize}
\subsubsection{Controllo dei cicli con UnionFind}
L'algoritmo mantiene una foresta di alberi ($A$) che man mano vengono fusi dall'aggiunta di archi. Si crea un ciclo se 
viene aggiunto un arco tra due nodi che appartengono allo stesso albero. Si può usare la \textbf{UnionFind} per identificare 
gli insiemi di vertici che già appartengono allo stesso albero. Ogni qualvolta si vuole aggiungere un arco $(u,v)$, di 
considera find(u) e find(v): se sono uguali, $u$ e $v$ appartengono allo stesso albero.
\begin{algorithm}
    \caption{KRUSKAL CON UNIONFIND(G)}
    \begin{algorithmic}
        \State $A\gets$ insieme di archi vuoto 
        \State oridina gli archi in una sequenza $S$ in oridne non decrescente di peso 
        \State crea una UnionFind contenente i vertici di $G$ come insiemi iniziali 
        \For{ongi arco $(u,v)\in S$}
            \If{$find(u)\neq find(v)$}
                \State aggiungi $(u,v)$ ad $A$
                \State union(u,v)
            \EndIf 
        \EndFor
    \end{algorithmic}
\end{algorithm}
\subsubsection{Complessità}
La complessità è pari a $\mathcal{O}(mlogn)$, come l'algoritmo di Prim.
\subsubsection{Correttezza}
\paragraph*{Invariante}
Gli archi in $A$ definiscono una foresta che è sottoinsieme di un certo MAR.
\paragraph*{Caso base}
L'invariante è banalmente vero all'istante iniziale, quando $A$ non contiene nessun arco.
\paragraph*{Passo}
\subparagraph*{Caso 1}
Si sceglie $(u,v)$ come l'arco più leggero non ancora considerato. Nel primo caso, $u$ e $v$ appartengono allo stesso 
albero; l'aggiunta di $(u,v)$ creerebbe un ciclo, quindi viene scartato.
\subparagraph*{Caso 2}
Nel secondo caso, $u$ e $v$ appartengono a due alberi distinti $T_1$ e $T_2$. Consideriamo un taglio che non tagli nessun 
arco di $A$, e che abbia $T_1$ e $T_2$ da parti opposte, tagliando l'arco $(u,v)$. L'arco $(u,v)$ è l'arco di peso minimo 
fra tutti gli archi che attraversano il taglio. Quindi, per il lemma del taglio, sappiamo che $(u,v)\cup A$ è un sottoinsieme 
di un MAR.
\section{Programmazione dinamica}
\subsection{Tecnica della programmazione dinamica}
\subsubsection{Memoizzazione}
La \textbf{memoizzazione} consiste nel memorizzare i valori via via calcolati da un algoritmo in una struttura di supporto.
È possibile applicare la tecnica della memoizzazione a tutte quelle funzioni che non cambiano comportamento, sugli stessi 
input, nel tempo.
\subsubsection{Massimo Sottoinsieme Indipendente}
Si consideri un grafo $G$ con struttura lineare. Ad ogni nodo $v_i$ è associato un peso non negativo $w_i$. 
Dato un grafo con questa struttura, si determini il sottoinsieme indipendente di nodi del grafo ($S$) che ha massimo peso. 

Questo problema gode della proprietà della sottostruttura ottima.
\begin{theorem}[Sottostruttura ottima MSI]
    Se $S_i$ è una soluzione ottima per $P_i$ (problema ristretto ai primi $i$ nodi), allora vale una delle seguenti affermazioni:
    \begin{itemize}
        \item $V_i \notin S_i$ e $S_i$ è anche una soluzione ottima per $P_{i-1}$
        \item $V_i \in S_i$ e $S_i-\{V_i\}$ è una soluzione attima per $P_{i-2}$
    \end{itemize}
\end{theorem}
\begin{proof}
    Se $S_i$ è una soluzione ottima per $P_i$, allora sono possibili due casi.

    \textbf{Caso 1}. Nel primo caso, $S_i$ è anche una soluzione ammissibile per il problema $P_{i-1}$, in quanto non contiene 
    $V_i$. Se non fosse ammissibile per $P_{i-1}$, esisterebbe una soluzione $S'_{i-1}$ con peso maggiore di $S_i$, ma tale 
    soluzione sarebbe una soluzione anche per $P_i$ e sarebbe migliore di $S_i$, contraddicendo l'ipotesi che $S_i$ sia 
    la soluzione ottima per $P_i$.

    \textbf{Caso 2}. Nel secondo caso, $V_i$ fa parte di $S_i$, quindi $S'=S_i-\{V_i\}$ è una soluzione ottima per il problema 
    $P_{i-2}$. $V_{i-1}$ non può appartenere a $S_i$ per la condizione di indipendenza sulle soluzioni. Per il problema 
    $P_{i-2}$, $S'$ è una soluzione ottima: se non lo fosse, esisterebbe una soluzione $S''$ per $P_{i-2}$ di peso maggiore 
    di $S'$. Allora $S''\cup \{V_i\}$ sarebbe una soluzione per $P_i$ e avrebbe peso maggiore di $S_i$ (assurdo). 
\end{proof}
\begin{corollary}
    Se $S_{i-1}$ è una soluzione ottima per $P_{i-1}$ e $S_{i-2}$ è una soluzione ottima per $P_{i-2}$, allora la soluzione 
    per $P_i$ è la soluzione di peso massimo tra $S_{i-1}$ e $S_{i-2}\cup \{V_i\}$.
\end{corollary}
\begin{equation*}
    \begin{split}
        S_0=\emptyset\\
        S_1=w_1\\
        S_i = 
        \left\{
            \begin{array}{lr}
                S_{i-1}&\text{se } W(S_{i-1})> W(S_{i-1})+w_i\\
                S_{i-2}\cup V_i&\text{altrimenti}
            \end{array}
        \right\}
    \end{split}
\end{equation*}
Si può calcolare $S_i$ a partire dai sottoproblemi $S_{i-1}$ e $S_{i-2}$.
La proprietà di sottostruttura ottima è fondamentale per poter esprimere le funzioni in una forma che permetta la memoizzazione.
\begin{algorithm}
    \caption{MSI(n,A)}
    \begin{algorithmic}
        \State $A[0]\gets \{\}$
        \State $A[1]\gets \{V_1\}$
        \For{i=2...n}
            \State $A[i]\gets$maxPeso($A[i-1], A[i-2]+V_i$)
        \EndFor\\
        \Return $A[n]$
    \end{algorithmic}
\end{algorithm}
\subsection{Longest Common Subsequence}
Data una sequenza $S:a_1,\dots,a_m$, una \textbf{sottosequenza} di $S$ è una qualsiasi sequenza ottenuta da un $S$ togliendo 
alcuni elementi. La sottosequenza deve rispettare l'ordine degli elementi della sequenza originale.
\subsubsection{Definizione del problema}
Date due sequenze $S1$ e $S2$, trovare la più lunga sequenza $S3$ che è sottosequenza sia di $S1$ che di $S2$. Notatione:
$S3=lcs(S1,S2)$.
\subsubsection{Sottostruttura ottima}
Siano $S1:a_1,\dots,a_m$ e $S2:b_1,\dots,b_n$ e $S3:c_1,\dots,c_k$; sono possibili due casi.

\textbf{Caso 1}. Nel primo caso, $a_m=b_n$. $a_m$ sarà contenuto in $S3$, e occuperà l'ultima posizione di $S3$; quindi 
$c_k=a_m=b_n$. Dato che gli ultimi elementi della sequenze iniziali già appartengono alla lcs, si possono non considerare
nel confronto con il resto delle sequenze. Quindi $S3=lcs(S1_{m-1},S2_{n-1})+\{a_m\}$.

\textbf{Caso 2}. Nel secondo caso, $a_m\neq b_n$. In questo caso, possiamo dure che $a_m$ o $b_n$ non saranno contenuti 
in $S3$. Quindi, $S3$ sarà la sequenza più lunga tra $lcs(S1-\{a_m\},S2)$ e $lcs(S1,S2-\{b_n\})$.
\subsubsection{Struttura per memoizzazione}
Per memorizzare la soluzione di tutti i sottoproblemi, si usa una \textbf{matrice} (LCS) di $m+1$ righe e $n+1$ colonne. La 
casella $LCS[i,j]$ contiene la più lunga sottosequenza comune dei due segmenti iniziali di lunghezze rispettive $i$ e $j$.
La casella 0-esima contiene la più lunga sottosequenza dei prefissi vuoti.
\subsubsection{Ottimizzazioni}
Per ottimizzare l'operazione di controllo sulla lunghezza delle lcs memoizzate, si può definire una nuova matrice $L$ che 
mantenga in $L[i,j]$ la lunghezza delle lcs memoizzate.
Ogni volta, $LCS[i,j]$ è uguale a $LCS[i-1,j-1]+a[i-1]$ oppure $LCS[i-1,j]$ oppure $LCS[i,j-1]$: è possibile utilizzare 
semplicemente delle frecce per indicare come si sta costruendo $LCS[i,j]$, senza dover copiare ogni volta l'elemento precendente.
\begin{algorithm}
    \caption{LCS(a,b,m,n)}
    \begin{algorithmic}
        \State $LCS\gets$ nuova matrice di dimensioni $m+1*n+1$
        \State $L\gets$ nuova matrice di dimensioni $m+1*n+1$
        \For{$i=0\dots m$}
            \State $LCS[i,0]\gets []$  
            \State $L[i,0]\gets 0$
        \EndFor 
        \For{$i=1\dots m$}
            \For{$j=1\dots m$}
                \If{$a[i-1]==b[j-1]$}
                    \State $LCS[i,j]\gets \nwarrow$
                    \State $L[i,j]\gets L[i-1,j-1]+1$
                \ElsIf{$L[i-1,j]>L[i,j-1]$}
                    \State $LCS[i,j]\gets \uparrow$
                    \State $L[i,j]\gets L[i-1,j]$
                \Else 
                    \State $LCS[i,j]\gets \leftarrow$
                    \State $L[i,j]\gets L[i,j-1]$
                \EndIf 
            \EndFor 
        \EndFor\\
        \Return $LCS[m,n]$
    \end{algorithmic}
\end{algorithm}
\subsubsection{Complessità}
Lc costruzione della matrice ha costo $\mathcal{O}(mn)$, così come il suo popolamento. La ricostruzione della soluzione 
ha costo $\mathcal{O}(m+n)$: in totale, $\mathcal{O}(mn)$.
\subsection{Zaino 0-1}
Simile al problema dello zaino frazionario, ma con oggetti non frazionabili.
\subsubsection{Costruzione di algoritmi di programmazione dinamica}
Passi da seguire per cotruire un algoritmo di programmazione dinamica:
\begin{enumerate}
    \item descrivere la \textbf{struttura dati} necessaria per la \textbf{memoizzazione}
    \item definire i \textbf{casi base} e le loro soluzioni banali
    \item scrivere l'algoritmo che inizializzi la struttura di memoizzazione seguendo i casi base e successivamente la 
    popoli in maniera bottom-up; infine restituire la soluzione 
\end{enumerate}
\subsubsection{Costruzione zaino 0-1}
Equazione ricorsiva per l'algoritmo:
\begin{equation*}
    V(i,j)=
    \left\{
        \begin{array}{lr}
            V(i-1,j)&\text{se } j<p_i\\
            max(V(i-1,j), v(i-1,j-p_i)+v_i)&\text{altrimenti}
        \end{array}
    \right\}
\end{equation*}
$V(i,j)$ ha due parametri: $i$ è l'ultimo oggetto considerato, $j$ è la capienza. La struttura di memoizzazione è quindi 
una matrice; la soluzione sarà contenura in $V[n,P]$. La matrice avrà dimensioni $(n+1)*(P+1)$ per includere i casi base. 

Casi base:
\begin{itemize}
    \item $i=0$: nessun oggetto considerato, $V[0,j]$ per ogni $0\leq j\leq P$
    \item $j=0$: capienza 0, $V[i,0]=0$ per ogni $0\leq i\leq n$
\end{itemize}
Per sapere quali oggetti appartengono alla soluzione, basta utilizzare una matrice ausiliaria $K$.
\begin{algorithm}
    \caption{ZAINO(n,P,v[],p[])}
    \begin{algorithmic}
        \State $V[]\gets$ nuova matrice $(n+1)*(P+1)$
        \State $K[]\gets$ nuova matrice $(n+1)*(P+1)$
        \For{$i=0\dots n$}
            \State $V[i,0]=0$
            \State $K[i,0]=0$
        \EndFor
        \For{$j=0\dots n$}
            \State $V[0,j]=0$
            \State $K[0,j]=0$
        \EndFor 
        \For{$i=1\dots n$}
            \For{$j=1\dots P$}
                \State $V[i,j]=V[i-1,j]$
                \State $K[i,j]=0$
                \If{$V[i,j]<V[i-1,j-p[i]]+v[i]$}
                    \State $V[i,j]<V[i-1,j-p[i]]+v[i]$
                    \State $K[i,j]=1$
                \EndIf 
            \EndFor 
        \EndFor\\
        \Return $V[n,P]$
    \end{algorithmic}
\end{algorithm}
\section{UnionFind}
\subsection{Definizione del problema}
Il problema è mantenere una collezione di insiemi disgiunti sulla quale siano possobili le seguenti operazioni: 
\begin{itemize}
    \item \textbf{union(A,B)}: fonde gli insiemi $A$ e $B$ in un unico insieme $A \cup B$
    \item \textbf{find(x)}: restituisce il nome dell'insieme contenente l'elemento $x$
    \item \textbf{makeSet(x)}: crea il nuovo insieme $\{x\}$, avente $x$ come unico elemento
\end{itemize}
Gli insiemi rimangono sempre disgiunti. Per distinguere tra diversi insiemi, si può individuare un elemento rappresentante,
cosicchè union(a,b) fonda i due insiemi rappresentati dagli elementi $a$ e $b$.
\subsection{Implementazione}
2 tipi di approcci fondamentali:
\begin{itemize}
    \item privilegiare esecuzione efficiente dell'operazione di find (\textbf{QuickFind})
    \item privilegiare esecuzione efficiente dell'operazione di union (\textbf{QuickUnion})
\end{itemize}
\subsubsection{QuickFind}
Gli approcci di tipo \textbf{QuickFind} utilizzano alberi con due livelli per realizzare la UnionFind. La radice di ogni 
albero contiene il rappresentante di un insieme, le foglie rappresentano gli elementi dell'insieme. Il rappresentante è 
contenuto sia nella radice sia in una foglia.

Operazioni su QuickFind:
\begin{itemize}
    \item makeSet(x): ccrea un nuovo albero composto da una radice ed una foglia, ed in entrambi posiziona $x$. Costo: 
    $\mathcal{O}(1)$.
    \item find(x): accede alla foglia corrispondente all'elemento $x$, risale di un livello incontrando la radice e restituisce 
    l'elemento contenuto in essa. $\mathcal{O}(1)$.
    \item union(a,b): considera l'albero $A$ contenente $a$ e $B$ contenente $b$, per ogni forglia di $B$, sostituisce il 
    puntatore al padre con un puntatore alla radice di $A$ e cancella la radice di $B$. $\mathcal{O}(n)$.
\end{itemize}
\subsubsection{QuickUnion}
Gli approcci di tipo \textbf{QuickUnion} utilizzano alberi con più di 2 livelli per rappresentare le UnionFind. La radice 
di ogni albero contiene il rappresentante di un insieme. I nodi rappresentano gli elementi dell'insieme. Il rappresentante 
è conenuto solamente nella radice.

Operazioni su QuickUnion:
\begin{itemize}
    \item makeSet(x): crea un nuovo albero composto da un unico nodi contenente $x$. Costo: $\mathcal{O}(1)$.
    \item find(x): accede alla foglia corrispondente all'elemento $x$, risale fino alla radice dell'albero e restituisce 
    l'elemento contenuto nella radice. Costo: $\mathcal{O}(n)$.
    \item union(a,b): considera l'albero $A$ contenente $a$ e $B$ contenente $b$, rende la radice di $B$ figlio della radice 
    di $A$. Costo: $\mathcal{O}(1)$.
\end{itemize}
\subsection{Complessità e costo ammortizzato}
Senza bilanciamenti sull'albero, bisogna considerare il caso peggiore, $\mathcal{O}(n)$.
Ciò che interessa è il costo totale delle operazioni, ossia il \textbf{costo ammortizzato}.
\subsubsection{Metodo dei crediti}
Il \textbf{metodo dei crediti} viene usato per determinare il costo ammotizzato di una sequenza di operazioni, senza andare 
nel dettaglio. 1 credito vale $\mathcal{O}(1)$ passi di esecuzione. Le funzioni meno costose depositano crediti sugli oggetti,
ed i crediti possono essere prelevati dalle funzioni più costose. Il costo ammortizzato è dato dalla somma di tutti i costi 
diviso per il numero di operazioni.

Vediamo $n$ makeSet e $n-1$ makeUnion analizzati con il metodo dei crediti:
\begin{itemize}
    \item \textbf{Numero massimo di cambi padre}. Ad ogni makeUnion, se una foglia cambia padre, il nuovo insieme che la
    contiene sarà grande almeno il doppio di quello di partenza. Quindi la foglia dopo $k$ cambi di padre apparterrà ad 
    un insieme di $2^k$ elementi. Siccome il numero totale di elementi è $n$, $2^k\leq n$, quindi $k\leq \log_2n$.
    \item \textbf{Deposito crediti}. Assegniamo ad ogni makeSet un costo aggiuntivo di $\log_2n$ crediti. Con $n$ makeSet, 
    si accumulano $n\log_2n$ crediti.
    \item \textbf{Costo union}. Ogni union consuma 1 credito per il costo dell'operazione d cambio del padre. Poichè i cambi 
    di padre sono limitati a $\log_2n$ per foglia, si consumano un totale di $n\log_2n$ crediti.
\end{itemize}
Costo di $m$ find, $n$ makeSet e $n-1$ makeUnion: $\mathcal{O}(m+n\log_2n)$.
\subsection{Bilanciamento su QuickFind}
Per ridurre il costo dell'operazione di union, si considera come insieme primario quello con cardinalità maggiore, e modificare 
il padre delle foglie dell'altro insieme. Il valore \textbf{size(x)} memorizza la cardinalità dell'insieme $x$: quando 
viene svolta la union, si controlla size(x) per identificare l'insieme primario.
\subsection{Bilanciamento su QuickUnion}
Sono possibili 2 ottimizzazioni sulla QuickUnion:
\begin{itemize}
    \item \textbf{Union by rank}: nell'unione degli insiemi $A$ e $B$, rendiamo la radice dell'albero più basso figlia della 
    radice dell'albero più alto 
    \item \textbf{Union by size}: nell'unione degli insiemi $A$ e $B$, rendiamo la radice dell'albero con meno nodi figlia 
    della radice dell'albero con più nodi 
\end{itemize}
\subsubsection{QuickUnion by rank}
Introduciamo un parametro \textbf{rank} che tenga conto dei livelli dell'albero. Quando facciamo una union, se 
$rank(B)<rank(A)$, rendiamo la radice di $B$ figlio della radice di $A$; per $rank(A)<rank(B)$, viceversa.

Dimostriamo che l'albero con radice $x$ ha almeno $2^{rank(x)}$ nodi.
\begin{proof}
    \textbf{Caso base}. $rank(A)=0$ ed un solo nodo. Allora $|A|=1\geq 2^0=2^{rank(A)}$

    \textbf{Passo}. Dopo ogni union:
    \begin{itemize}
        \item se $rank(A)>rank(B)$: $|A\cup B|=|A|+|B|=2^{rank(A\cup B)}$
        \item se $rank(B)>rank(A)$: simmetrico al precedente 
        \item se $rank(A)=rank(B)$: $|A\cup B|=|A|+|B|=2^{rank(A\cup B)}$
    \end{itemize}
    Il numero massimo di nodi è $n$, quindi $2^{rank(x)}\leq n$ e $rank(x)\leq \log_2n$. La find richiede tempo $\mathcal{O}(\log n)$.
\end{proof}
\subsubsection{QuickUnion by size}
Utilizziamo il parametro $size(x)=$numero di nodi dell'albero di cui $x$ è radice.
Quando eseguiamo una union, se $size(B)\leq size(A)$ rendiamo la radice di $B$ figlio della radice di $A$, invece se 
$size(B)<size(A)$ rendiamo la radice di $A$ figlio della radice di $B$, e scambiamo le due radici. Infine, $size(A)=size(A)+size(B)$.
\subsection{Compressione nell'operazione find}
Le euristiche di compressione vengono applicate durante l'esecuzione di una find, ed hanno lo scopo di diminuire l'altezza
dell'albero. Ne vediamo tre tipi:
\begin{itemize}
    \item \textbf{path compression}: ad ogni find prendiamo il reiferimento di ogni nodo radice e poi li facciamo diventare 
    figli dell'ultimo nodo radice
    \item \textbf{path splitting}: rendo un nodo figlio di suo nonno
    \item \textbf{path halving}: come path halving ma con un solo puntatore
\end{itemize}
Combinando le euristiche di bilanciamento e compressione, una qualunque sequenza di $n$ makeSet, $m$ find e $n-1$ union 
può essere eseguita in $\mathcal{O}((n+m)log^*)$.
\section{Programmazione dinamica applicata ai grafi}
\subsection{Algoritmo di Bellman-Ford}
\subsubsection{Condizione di Bellman}
Per ogni arco $(u,v)$ e per ogni vertice $s$ si ha $\delta(s,v)\leq \delta(s,u)+W(u,v)$, per la \textbf{disuguaglianza 
triangolare delle distanze}.
\begin{lemma}
    Un arco $(u,v)$ appartiene ad un cammino minimo da $s$ a $v$ se e solo se $u$ è raggiungibile da $s$ e la condizione 
    di Bellman è soddisfatta con l'uguaglianza per $(u,v)$, cioè se vale $\delta(s,v)=\delta(s,u)+W(u,v)$.
\end{lemma}
\subsubsection{Tecnica del rilassamento}
Consideriamo una \textbf{stima} $D(s,v)$ \textbf{per eccesso di} $\delta(s,v)$, che sia il peso di un cammino sul grafo 
tra $s$ e $v$ (quindi, $D(s,v)\geq \delta(s,v)$).
Supponiamo esista un arco $(u,v)$ per cui valga $D(s,v)>D(s,u)+W(u,v)$. Per rendere vera la disuguaglianza triangolare, 
possiamo decrementare $D(s,v)$ con l'assegnazione $D(s,v)\gets D(s,u)+W(u,v)$: questa operazione è detta \textbf{rilassamento 
dell'arco}.

Applicando esaustivamente l'operazione di rilassamento, per grafi senza cicli negativi, $D(s,v)$ converge a $\delta(s,v)$.
Nel momento in cui $D(s,v)=\delta(s,v)$, l'ultimo arco per cui è stato applicato il rilassamento è quello per cui la condizione
di Bellman vale con l'uguaglianza e $(u,v)$ appartiene ad un cammino minimo tra $s$ e $v$. 
\subsubsection{Programmazione dinamica per Bellman-Ford}
\begin{proprietà}
    Un sottocammino di un cammino minimo è un cammino minimo.
\end{proprietà}
Sia $\langle s,v_1,v_2,\dots,v_k \rangle$ un cammino minimo da $s$ a $v_k$. Grazie alla condizione di Bellman, possiamo 
scrivere: $\delta(s,v_k)=\delta(s,v_{k-1})+W(v_{k-1},v_k)$.

Sappiamo che il cammino minimo $\langle s,v_1,v_2,\dots,v_{k-1} \rangle$ avrà lunghezza $k-1$, quindi il sottoproblema di 
trovare $\langle s,v_1,v_2,\dots,v_{k-1} \rangle$ dovrà considerare solo cammini di lunghezza $k-1$ e così via fino a $k=0$.

Usiamo le stime $D(s,v)$ come struttura di memiozzazione, ed inizializziamo $D(s,s)=0$ e $D(s,v)=\infty$ per tutti gli 
altri vertici $v$ del grafo, rispettando la condizione per cui $D(s,v)\geq \delta(s,v)$.

Applicando il rilassamento su tutti gli archi una sola volta, troveremo: $D(s,v_1)>D(s,s)+W(s,v_1)$ (poichè $W(s,v_1)<\infty$).
Il rilassamento porrà $D(s,v_1)=D(s,s)+W(s,v_1)$. Ma $D(s,s)=\delta(s,s)$, e $\langle s,v_1 \rangle$ è un cammino minimo, 
quindi $D(s,v_1)=\delta(s,s)+W(s,v_1)=\delta(s,v_1)$.  

Iterando il processo, al k-esimo passo avremo $D(s,v_k)=\delta(s,v_k)$.
Un cammino minimo semplice può contenere al massimo tutti i vertici del grafo, quindi $n-1$ archi. Di conseguenza, bastano 
$n-1$ iterazioni per trovare i cammini minimi da $s$ a tutti i vertici del grafo.
\begin{algorithm}
    \caption{BELLMAN-FORD(G,W,s)}
    \begin{algorithmic}
        \State INIZIALIZZA(G)
        \State $d[s]\gets 0$
        \For{$i=1\dots n-1$}
            \For{ogni $(u,v)$ in $G$}
                \If{$d[v]>d[u]+W(u,v)$}
                    \State $\pi[v]\gets u$
                    \State $d[v]\gets d[u]+W(u,v)$
                \EndIf 
            \EndFor 
        \EndFor
    \end{algorithmic}
\end{algorithm}
\subsubsection{Correttezza}
Sia $\delta(s,v_k)$ a distanza da $s$ a $v_k$.

Definiamo l'invariante \textbf{KPATH}: dopo la k-esima iterazione del ciclo esterno, si ha $d[v_k]=\delta(s,v_k)$ per ogni 
nodo $v_k$ per cui il cammino minimo da $s$ a $v_k$ è composto al più da $k$ archi.
\begin{proof}
    \textbf{Base}. per $k=0$, per il cammino da $s$ a $s$, $d[s]=0$ e $d[u]=\infty$ per tutti gli altri nodi $u$. 
    \textbf{Passo}. Dimostriamo che KPATH sia vero anche dopo $k+1$ iterazioni. Sia $v_{k+1}$ un nodo il cui cammino minimo 
    $s\leadsto v_k\leadsto v_{k+1}$ è composto da $k+1$ archi. Sappiamo che $\delta(s,v_{k+1})=\delta(s,v_k)+W(v_k,v_{k+1})$
    e $d[v_k]=\delta(s,v_k)$. Quindi: $\delta(s,v_{k+1})=d[v_k]+W(v_k,v_{k+1})$.

    Al passo $k+1$, ci sono due casi:
    \begin{itemize}
        \item $d[v{k+1}]$ viene aggiornata, allora $d[v{k+1}]=\delta(s,v_{k+1})$
        \item $d[v{k+1}]$ non viene aggiornata, poichè $d[v{k+1}]$ è il peso di un cammino da $s$ a $v_{k+1}$ in $G$, 
        $d[v{k+1}]\geq \delta(s,v_{k+1})$, e poichè non è stata aggiornata, $d[v{k+1}]=\delta(s,v_{k+1})$.
    \end{itemize}
    I cammini restituiti dall'algoritmo sono effettivamente quelli minimi.
\end{proof}
\subsubsection{Complessità}
L'algoritmo da $n-1$ iterazioni del ciclo esterno. Per ogni iterazione, l'algoritmo considera tutti gli archi del grafo 
con una serie di operazioni di costo $\mathcal{O}(1)$. La complessità è quindi $\mathcal{O}(n*m)$

L'algoritmo di Dijkstra è più efficiente perchè sceglie $(u,v)$ garantendo che $(u,v)$ sia l'arco che minimizza $D(s,u)+W(u,v)$
tra tutti gli archi nei quali $u$ è stato raggiunto da un cammino minimo e $v$ no, compiendo $\mathcal{O}(m)$ operazioni 
di rilassamento; invece l'algoritmo di Bellman-Ford sceglie $(u,v)$ tra tutti gli archi presenti in $G$.
\subsubsection{Trovare cicli negativi}
Se un cammino minimo contiene vertici ripetuti, vuol dire che nel grafo esiste un ciclo.
Se il ciclo ha \textbf{peso positivo}, il cammino minimo è quello ottenuto percorrendolo una sola volta.
Se il ciclo ha \textbf{peso 0}, viene rilevato il cammino minimo ottenuto percorrendo il ciclo una volta sola.
Se il ciclo ha \textbf{peso negativo}, ogni volta che viene percorso viene trovato un cammino minimo più lungo e di peso 
minore: per i cicli negativi non sarà mai possibile trovare una soluzione ottima.
\begin{algorithm}
    \caption{BF-NEG(G,W,s)}
    \begin{algorithmic}
        \State INIZIALIZZA(G)
        \State $d[s]\gets 0$
        \For{$i=1\dots n-1$}
            \For{ogni $(u,v)$ in $G$}
                \If{$d[v]>d[u]+W(u,v)$}
                    \State $\pi[v]\gets u$
                    \State $d[v]\gets d[u]+W(u,v)$
                \EndIf 
            \EndFor
        \EndFor
        \For{ogni $(u,v)$ in $G$}
            \If{$d[v]>d[u]+W(u,v)$}\\
                \Return{errore}
            \EndIf
        \EndFor 
    \end{algorithmic}
\end{algorithm}
Per rilevare cicli negativi basta controllare se l'n-esima iterazione aggiorna qualche distanza: se così succede, esiste 
un ciclo negativo.
\subsubsection{Ottimizzazione di BF per DAG}
L'algoritmo di Bellman-Ford compie molti passi di rilassamento inutili. In particolare, l'assegnazione 
$D(s,v_k)\gets D(s,v_{k-1})+W(v_{k-1},v_k)$ è inutile se $D(s,v_{k-1})>\delta(s,v_{k-1})$: infatti, sarà poi necessario 
almeno un ulteriore rilassamento dello stesso arco per calcolare $\delta(s,v_{k})$.
Se un grafo è un DAG, si può calcolare prima un ordinamento topologico, e poi passare tutti gli archi una sola volta seguendo
l'ordine topologico.
\begin{algorithm}
    \caption{BF-DAG(G,W,s)}
    \begin{algorithmic}
        \State INIZIALIZZA(G)
        \State $d[s]\gets 0$
        \State $ord\gets$ TOPOLOGICAL-SORT(G)
        \For{$i=1\dots n-1$}
            \For{ogni $(ord[i],v)$}
                \If{$d[v]>d[ord[i]]+W(ord[i],v)$}
                    \State $\pi[v]\gets ord[i]$
                    \State $d[v]\gets d[ord[i]]+W(ord[i],v)$
                \EndIf 
            \EndFor
        \EndFor
    \end{algorithmic}
\end{algorithm}
Ha complessità $\mathcal{O}(m+n)$.
\subsection{Algoritmo di Floyd-Warshall}
\subsubsection{Distanze e cammini minimi k-vincolati}
Denotiamo i vertici del grafo come $v1,v2,\dots,v_n$. Per un $k$ fissato con $1\leq k\leq n$ definiamo: \textbf{cammino 
minimo k-vincolato} tra $x$ e $y$ (denotato con $\pi^k_{xy}$) il cammino che va da $x$ a $y$ di costo minimo tra tutti
quelli che non contengono i vertici $\{v_{k+1},\dots,v_n\}$ e \textbf{distanza k-vincolata} (denotata con $d^k_{xy}$), 
che è il peso $W$ se $\pi^k_{xy}$ esiste, $\infty$ altrimenti.

Un cammino minimo k-vincolato è un cammino minimo in un \textbf{grafo k-vincolato} $G_k$: varrà quindi la proprietà della 
sottostruttura ottima e sarà possibile applicare tecniche di programmazione dinamica. 


Per ogni $1\leq k\leq n$ e per ogni coppia $x,y$ definiamo l'equazione ricorsiva: 
\begin{equation*}
    d^k_{xy}=min(d^{k-1}_{xy}, d^{k-1}_{xv_k}+d^{k-1}_{v_ky})
\end{equation*}
In poche parole, preso un $k$, si va a vedere se la concatenazione dei cammini minimi da $x$ a $v_k$ e da $v_k$ a $y$ ha 
peso minore del cammino da $x$ a $y$ e si verifica la disuguaglianza triangolare: se \textbf{non} si verifica, si applica 
un rilassamento facendo passare il cammino minimo attraverso $v_k$.
Abbiamo due casi:
\begin{itemize}
    \item $v_k\notin\pi^k_{xy}$: allora $\pi^k_{xy}$ è anche un cammino minimo $(k-1)$ vincolato; se così non fosse, esisterebbe un 
    cammino minimo $(k-1)$ vincolato di peso minore, ma questo sarebbe anche un cammino k-vincolato con peso minore di $\pi^k_{xy}$
    (assurdo)
    \item $v_k\in\pi^k_{xy}$: i sottocammini da $x$ a $v_k$ e da $v_k$ a $y$ sono cammini minimi k-vincolati (per sottostruttura 
    ottima); poichè non contengono internamente $v_k$, essi sono anche cammini $(k-1)$ vincolati.
\end{itemize}
\subsubsection{Struttura di memoizzazione}
Per modellare, per ogni coppia di vertici $x$ e $y$, la lunghezza del cammino provvisorio tra i due vertici, per ogni 
k-esima iterazione dell'algoritmo, servono $n$ matrici $D^k$ di dimensione $n\times n$ (lo spazio richiesto è $\mathcal{O}(n^3)$). 

$D^{k-1}[i,j]$ contiene la distanza tra il vertice $v_i$ e $v_j$ in un grafo $G^{k-1}$ in cui 
\end{document}